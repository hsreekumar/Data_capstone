{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Simplest NLP Tutorial(Random Forest & Naive Bayes)",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e026ec7479514d18a3ab79aecfce542d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_968122e36c864963b548850357465767",
              "IPY_MODEL_f86e90a15f1d4b36811af6744483a0f1",
              "IPY_MODEL_aac8607e3ac3484a8b0e28c09d9ca3ac"
            ],
            "layout": "IPY_MODEL_f92003dfcd804988b148017aaaf3042a"
          }
        },
        "968122e36c864963b548850357465767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58a7c7eaeb234005b874fa877128905b",
            "placeholder": "​",
            "style": "IPY_MODEL_9679a7b3319743dfa8339a1a1feb6a33",
            "value": "vocab.txt: 100%"
          }
        },
        "f86e90a15f1d4b36811af6744483a0f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4287bf07fd646dab37e0f6634a319b8",
            "max": 226122,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7858c985f6d6455383882179aaa23a0d",
            "value": 226122
          }
        },
        "aac8607e3ac3484a8b0e28c09d9ca3ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6796894e34f402882ce804a252e16c9",
            "placeholder": "​",
            "style": "IPY_MODEL_1b8fe9d49e514bde9ab141adb6772b8c",
            "value": " 226k/226k [00:00&lt;00:00, 517kB/s]"
          }
        },
        "f92003dfcd804988b148017aaaf3042a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58a7c7eaeb234005b874fa877128905b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9679a7b3319743dfa8339a1a1feb6a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4287bf07fd646dab37e0f6634a319b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7858c985f6d6455383882179aaa23a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6796894e34f402882ce804a252e16c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b8fe9d49e514bde9ab141adb6772b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc9b7de2eae74f9ba6ee9b487cd6a1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ade1c9ecc0674781bab72d5072b756c7",
              "IPY_MODEL_dc0391c1f0e84c0abaa1bdbc382ecccc",
              "IPY_MODEL_84ec8168402e4455afea2b91791c45fb"
            ],
            "layout": "IPY_MODEL_42c27703b3dd4a2da0d5bc3fe805fbf7"
          }
        },
        "ade1c9ecc0674781bab72d5072b756c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6900716ed6804304bb2823279ba86ab2",
            "placeholder": "​",
            "style": "IPY_MODEL_78455f98eb9245d6ad500f859bd2bd1b",
            "value": "config.json: 100%"
          }
        },
        "dc0391c1f0e84c0abaa1bdbc382ecccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_033fb228b0364e1f81fcda263929ac78",
            "max": 533,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d527fc5b7cd74a309c21db6ff1180444",
            "value": 533
          }
        },
        "84ec8168402e4455afea2b91791c45fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_387f17286656492e85e8a942390660d5",
            "placeholder": "​",
            "style": "IPY_MODEL_45469eaf2b014dcd874fc0ec3ee4e3f9",
            "value": " 533/533 [00:00&lt;00:00, 48.5kB/s]"
          }
        },
        "42c27703b3dd4a2da0d5bc3fe805fbf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6900716ed6804304bb2823279ba86ab2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78455f98eb9245d6ad500f859bd2bd1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "033fb228b0364e1f81fcda263929ac78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d527fc5b7cd74a309c21db6ff1180444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "387f17286656492e85e8a942390660d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45469eaf2b014dcd874fc0ec3ee4e3f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93058efaa14a47eeb14f87fb7e8effc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e63a4f6562c47aca94f735e2b37ea87",
              "IPY_MODEL_d47dee0a15284d469c2f38998ec44928",
              "IPY_MODEL_cf02c5c153a34ec6bb657c158cf3e424"
            ],
            "layout": "IPY_MODEL_83ba25ec93b040cf998b367177c5c8a8"
          }
        },
        "6e63a4f6562c47aca94f735e2b37ea87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13f7920691f741cfaa2202bc5a58cba0",
            "placeholder": "​",
            "style": "IPY_MODEL_9e8a856d2ba2455abc3c538c82be7bda",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "d47dee0a15284d469c2f38998ec44928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_260ea9a5dc5543caaa2d77dcc5288ddc",
            "max": 439101405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c34cd23d9d00454c93fdb2fb9e12dcba",
            "value": 439101405
          }
        },
        "cf02c5c153a34ec6bb657c158cf3e424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37660e3aad1549e59a56c552f9343797",
            "placeholder": "​",
            "style": "IPY_MODEL_8b47e29c5c264068a2d7f99c9f13b504",
            "value": " 439M/439M [00:01&lt;00:00, 296MB/s]"
          }
        },
        "83ba25ec93b040cf998b367177c5c8a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f7920691f741cfaa2202bc5a58cba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e8a856d2ba2455abc3c538c82be7bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "260ea9a5dc5543caaa2d77dcc5288ddc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c34cd23d9d00454c93fdb2fb9e12dcba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37660e3aad1549e59a56c552f9343797": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b47e29c5c264068a2d7f99c9f13b504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsreekumar/Data_capstone/blob/main/Stock_News_Bert2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'stocknews:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F129%2F792900%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240728%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240728T193814Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D24666fe8a099752eee2d3dacbec32f4c993587765f94423f4888c6b29a0d5babb49e1e66a777515ca87df47696b110fe737110573612f4251511deac467c317fdb28a2347310a12aa88579ca5c62984043a7bab0978b69e1a0d044a1d743bc887085500e5a539d3b49dce7f1cde220f10172f65f0c8aa736f0dd45a3b448790462e853163dee2e49a82d54a8cdaea1bc409c30f46328911eef310b3a7b7c2037174b192086decd20c86e2d573bded856f39ff757e65b9af0281de7df93ee26d785ce8a33d01128c6dbbe233b0d601940967acc769db9b2ec0da12edcdf862145a8daed1f9a2e76c32b42898c064d07da57759d3527f6e9cb2419d3c309ba367e'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73ZlSF_xfjLS",
        "outputId": "ee0414cc-80cb-42b5-defa-7a3fa562960a"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/129/792900/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240728%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240728T193814Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=24666fe8a099752eee2d3dacbec32f4c993587765f94423f4888c6b29a0d5babb49e1e66a777515ca87df47696b110fe737110573612f4251511deac467c317fdb28a2347310a12aa88579ca5c62984043a7bab0978b69e1a0d044a1d743bc887085500e5a539d3b49dce7f1cde220f10172f65f0c8aa736f0dd45a3b448790462e853163dee2e49a82d54a8cdaea1bc409c30f46328911eef310b3a7b7c2037174b192086decd20c86e2d573bded856f39ff757e65b9af0281de7df93ee26d785ce8a33d01128c6dbbe233b0d601940967acc769db9b2ec0da12edcdf862145a8daed1f9a2e76c32b42898c064d07da57759d3527f6e9cb2419d3c309ba367e to path /kaggle/input/stocknews\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stock Sentiment Analysis using news headlines.\n",
        "\n",
        "# Data set\n",
        "\n",
        "Daily News for Stock Market Prediction\n",
        "https://www.kaggle.com/datasets/aaron7sun/stocknews\n",
        "\n",
        "We will predict whether the stock price for a given stock will raise or drop based on the related news headlines.\n",
        "\n",
        "# Topic Covered\n",
        "\n",
        "Classifiers: Random Forest vs Naive Bayes\n",
        "\n",
        "Feature Transformation: Count Vectorizer vs TF-IDF Vectorizer\n",
        "\n",
        "![](https://media-exp1.licdn.com/dms/image/C5112AQFKtWwV4Gd2Lg/article-cover_image-shrink_600_2000/0/1549570474712?e=1654128000&v=beta&t=jPyk2pQW6L1bxbYJlpzve_BuRwGjoOUhxl5GUDK8Eac)\n"
      ],
      "metadata": {
        "id": "pMuzftV2fjLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "\n",
        "#Encoding are in general UTF-8(default setting), Latin-1 (also known as ISO-8859-1) or Windows-1251\n",
        "df=pd.read_csv('Combined_News_DJIA.csv', encoding = \"ISO-8859-1\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T13:55:59.370814Z",
          "iopub.execute_input": "2022-04-07T13:55:59.371227Z",
          "iopub.status.idle": "2022-04-07T13:55:59.482331Z",
          "shell.execute_reply.started": "2022-04-07T13:55:59.371186Z",
          "shell.execute_reply": "2022-04-07T13:55:59.481383Z"
        },
        "trusted": true,
        "id": "H_hl6rdBfjLU"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Inspection\n",
        "First, let's have a look at our data set...\n",
        "It seems our data is made of different news headlines related to a company and a label of wether the companies'stock price will increase or decrease.(0:decrease, 1: increase)"
      ],
      "metadata": {
        "id": "-coc8KPxfjLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T11:09:40.894032Z",
          "iopub.execute_input": "2022-04-07T11:09:40.894404Z",
          "iopub.status.idle": "2022-04-07T11:09:40.939296Z",
          "shell.execute_reply.started": "2022-04-07T11:09:40.894364Z",
          "shell.execute_reply": "2022-04-07T11:09:40.938558Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "LThWXqtefjLV",
        "outputId": "326fa8b2-ed50-47ee-8efa-ff03e374177f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date  Label                                               Top1  \\\n",
              "0  2008-08-08      0  b\"Georgia 'downs two Russian warplanes' as cou...   \n",
              "1  2008-08-11      1  b'Why wont America and Nato help us? If they w...   \n",
              "2  2008-08-12      0  b'Remember that adorable 9-year-old who sang a...   \n",
              "3  2008-08-13      0  b' U.S. refuses Israel weapons to attack Iran:...   \n",
              "4  2008-08-14      1  b'All the experts admit that we should legalis...   \n",
              "\n",
              "                                                Top2  \\\n",
              "0            b'BREAKING: Musharraf to be impeached.'   \n",
              "1        b'Bush puts foot down on Georgian conflict'   \n",
              "2                 b\"Russia 'ends Georgia operation'\"   \n",
              "3  b\"When the president ordered to attack Tskhinv...   \n",
              "4  b'War in South Osetia - 89 pictures made by a ...   \n",
              "\n",
              "                                                Top3  \\\n",
              "0  b'Russia Today: Columns of troops roll into So...   \n",
              "1  b\"Jewish Georgian minister: Thanks to Israeli ...   \n",
              "2  b'\"If we had no sexual harassment we would hav...   \n",
              "3  b' Israel clears troops who killed Reuters cam...   \n",
              "4  b'Swedish wrestler Ara Abrahamian throws away ...   \n",
              "\n",
              "                                                Top4  \\\n",
              "0  b'Russian tanks are moving towards the capital...   \n",
              "1  b'Georgian army flees in disarray as Russians ...   \n",
              "2  b\"Al-Qa'eda is losing support in Iraq because ...   \n",
              "3  b'Britain\\'s policy of being tough on drugs is...   \n",
              "4  b'Russia exaggerated the death toll in South O...   \n",
              "\n",
              "                                                Top5  \\\n",
              "0  b\"Afghan children raped with 'impunity,' U.N. ...   \n",
              "1      b\"Olympic opening ceremony fireworks 'faked'\"   \n",
              "2  b'Ceasefire in Georgia: Putin Outmaneuvers the...   \n",
              "3  b'Body of 14 year old found in trunk; Latest (...   \n",
              "4  b'Missile That Killed 9 Inside Pakistan May Ha...   \n",
              "\n",
              "                                                Top6  \\\n",
              "0  b'150 Russian tanks have entered South Ossetia...   \n",
              "1  b'What were the Mossad with fraudulent New Zea...   \n",
              "2  b'Why Microsoft and Intel tried to kill the XO...   \n",
              "3  b'China has moved 10 *million* quake survivors...   \n",
              "4  b\"Rushdie Condemns Random House's Refusal to P...   \n",
              "\n",
              "                                                Top7  \\\n",
              "0  b\"Breaking: Georgia invades South Ossetia, Rus...   \n",
              "1  b'Russia angered by Israeli military sale to G...   \n",
              "2  b'Stratfor: The Russo-Georgian War and the Bal...   \n",
              "3  b\"Bush announces Operation Get All Up In Russi...   \n",
              "4  b'Poland and US agree to missle defense deal. ...   \n",
              "\n",
              "                                                Top8  ...  \\\n",
              "0  b\"The 'enemy combatent' trials are nothing but...  ...   \n",
              "1  b'An American citizen living in S.Ossetia blam...  ...   \n",
              "2  b\"I'm Trying to Get a Sense of This Whole Geor...  ...   \n",
              "3             b'Russian forces sink Georgian ships '  ...   \n",
              "4  b'Will the Russians conquer Tblisi? Bet on it,...  ...   \n",
              "\n",
              "                                               Top16  \\\n",
              "0  b'Georgia Invades South Ossetia - if Russia ge...   \n",
              "1  b'Israel and the US behind the Georgian aggres...   \n",
              "2  b'U.S. troops still in Georgia (did you know t...   \n",
              "3                      b'Elephants extinct by 2020?'   \n",
              "4  b'Bank analyst forecast Georgian crisis 2 days...   \n",
              "\n",
              "                                               Top17  \\\n",
              "0                b'Al-Qaeda Faces Islamist Backlash'   \n",
              "1  b'\"Do not believe TV, neither Russian nor Geor...   \n",
              "2       b'Why Russias response to Georgia was right'   \n",
              "3  b'US humanitarian missions soon in Georgia - i...   \n",
              "4  b\"Georgia confict could set back Russia's US r...   \n",
              "\n",
              "                                               Top18  \\\n",
              "0  b'Condoleezza Rice: \"The US would not act to p...   \n",
              "1  b'Riots are still going on in Montreal (Canada...   \n",
              "2  b'Gorbachev accuses U.S. of making a \"serious ...   \n",
              "3             b\"Georgia's DDOS came from US sources\"   \n",
              "4  b'War in the Caucasus is as much the product o...   \n",
              "\n",
              "                                               Top19  \\\n",
              "0  b'This is a busy day:  The European Union has ...   \n",
              "1    b'China to overtake US as largest manufacturer'   \n",
              "2         b'Russia, Georgia, and NATO: Cold War Two'   \n",
              "3  b'Russian convoy heads into Georgia, violating...   \n",
              "4  b'\"Non-media\" photos of South Ossetia/Georgia ...   \n",
              "\n",
              "                                               Top20  \\\n",
              "0  b\"Georgia will withdraw 1,000 soldiers from Ir...   \n",
              "1                     b'War in South Ossetia [PICS]'   \n",
              "2  b'Remember that adorable 62-year-old who led y...   \n",
              "3  b'Israeli defence minister: US against strike ...   \n",
              "4  b'Georgian TV reporter shot by Russian sniper ...   \n",
              "\n",
              "                                               Top21  \\\n",
              "0  b'Why the Pentagon Thinks Attacking Iran is a ...   \n",
              "1  b'Israeli Physicians Group Condemns State Tort...   \n",
              "2          b'War in Georgia: The Israeli connection'   \n",
              "3                     b'Gorbachev: We Had No Choice'   \n",
              "4  b'Saudi Arabia: Mother moves to block child ma...   \n",
              "\n",
              "                                               Top22  \\\n",
              "0  b'Caucasus in crisis: Georgia invades South Os...   \n",
              "1  b' Russia has just beaten the United States ov...   \n",
              "2  b'All signs point to the US encouraging Georgi...   \n",
              "3  b'Witness: Russian forces head towards Tbilisi...   \n",
              "4   b'Taliban wages war on humanitarian aid workers'   \n",
              "\n",
              "                                               Top23  \\\n",
              "0  b'Indian shoe manufactory  - And again in a se...   \n",
              "1  b'Perhaps *the* question about the Georgia - R...   \n",
              "2  b'Christopher King argues that the US and NATO...   \n",
              "3  b' Quarter of Russians blame U.S. for conflict...   \n",
              "4  b'Russia: World  \"can forget about\" Georgia\\'s...   \n",
              "\n",
              "                                               Top24  \\\n",
              "0  b'Visitors Suffering from Mental Illnesses Ban...   \n",
              "1                 b'Russia is so much better at war'   \n",
              "2                        b'America: The New Mexico?'   \n",
              "3  b'Georgian president  says US military will ta...   \n",
              "4  b'Darfur rebels accuse Sudan of mounting major...   \n",
              "\n",
              "                                               Top25  \n",
              "0           b\"No Help for Mexico's Kidnapping Surge\"  \n",
              "1  b\"So this is what it's come to: trading sex fo...  \n",
              "2  b\"BBC NEWS | Asia-Pacific | Extinction 'by man...  \n",
              "3  b'2006: Nobel laureate Aleksander Solzhenitsyn...  \n",
              "4  b'Philippines : Peace Advocate say Muslims nee...  \n",
              "\n",
              "[5 rows x 27 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fcb6631a-7be3-4ff1-9cf3-1ff930729d64\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Label</th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>...</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-08-08</td>\n",
              "      <td>0</td>\n",
              "      <td>b\"Georgia 'downs two Russian warplanes' as cou...</td>\n",
              "      <td>b'BREAKING: Musharraf to be impeached.'</td>\n",
              "      <td>b'Russia Today: Columns of troops roll into So...</td>\n",
              "      <td>b'Russian tanks are moving towards the capital...</td>\n",
              "      <td>b\"Afghan children raped with 'impunity,' U.N. ...</td>\n",
              "      <td>b'150 Russian tanks have entered South Ossetia...</td>\n",
              "      <td>b\"Breaking: Georgia invades South Ossetia, Rus...</td>\n",
              "      <td>b\"The 'enemy combatent' trials are nothing but...</td>\n",
              "      <td>...</td>\n",
              "      <td>b'Georgia Invades South Ossetia - if Russia ge...</td>\n",
              "      <td>b'Al-Qaeda Faces Islamist Backlash'</td>\n",
              "      <td>b'Condoleezza Rice: \"The US would not act to p...</td>\n",
              "      <td>b'This is a busy day:  The European Union has ...</td>\n",
              "      <td>b\"Georgia will withdraw 1,000 soldiers from Ir...</td>\n",
              "      <td>b'Why the Pentagon Thinks Attacking Iran is a ...</td>\n",
              "      <td>b'Caucasus in crisis: Georgia invades South Os...</td>\n",
              "      <td>b'Indian shoe manufactory  - And again in a se...</td>\n",
              "      <td>b'Visitors Suffering from Mental Illnesses Ban...</td>\n",
              "      <td>b\"No Help for Mexico's Kidnapping Surge\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-08-11</td>\n",
              "      <td>1</td>\n",
              "      <td>b'Why wont America and Nato help us? If they w...</td>\n",
              "      <td>b'Bush puts foot down on Georgian conflict'</td>\n",
              "      <td>b\"Jewish Georgian minister: Thanks to Israeli ...</td>\n",
              "      <td>b'Georgian army flees in disarray as Russians ...</td>\n",
              "      <td>b\"Olympic opening ceremony fireworks 'faked'\"</td>\n",
              "      <td>b'What were the Mossad with fraudulent New Zea...</td>\n",
              "      <td>b'Russia angered by Israeli military sale to G...</td>\n",
              "      <td>b'An American citizen living in S.Ossetia blam...</td>\n",
              "      <td>...</td>\n",
              "      <td>b'Israel and the US behind the Georgian aggres...</td>\n",
              "      <td>b'\"Do not believe TV, neither Russian nor Geor...</td>\n",
              "      <td>b'Riots are still going on in Montreal (Canada...</td>\n",
              "      <td>b'China to overtake US as largest manufacturer'</td>\n",
              "      <td>b'War in South Ossetia [PICS]'</td>\n",
              "      <td>b'Israeli Physicians Group Condemns State Tort...</td>\n",
              "      <td>b' Russia has just beaten the United States ov...</td>\n",
              "      <td>b'Perhaps *the* question about the Georgia - R...</td>\n",
              "      <td>b'Russia is so much better at war'</td>\n",
              "      <td>b\"So this is what it's come to: trading sex fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-08-12</td>\n",
              "      <td>0</td>\n",
              "      <td>b'Remember that adorable 9-year-old who sang a...</td>\n",
              "      <td>b\"Russia 'ends Georgia operation'\"</td>\n",
              "      <td>b'\"If we had no sexual harassment we would hav...</td>\n",
              "      <td>b\"Al-Qa'eda is losing support in Iraq because ...</td>\n",
              "      <td>b'Ceasefire in Georgia: Putin Outmaneuvers the...</td>\n",
              "      <td>b'Why Microsoft and Intel tried to kill the XO...</td>\n",
              "      <td>b'Stratfor: The Russo-Georgian War and the Bal...</td>\n",
              "      <td>b\"I'm Trying to Get a Sense of This Whole Geor...</td>\n",
              "      <td>...</td>\n",
              "      <td>b'U.S. troops still in Georgia (did you know t...</td>\n",
              "      <td>b'Why Russias response to Georgia was right'</td>\n",
              "      <td>b'Gorbachev accuses U.S. of making a \"serious ...</td>\n",
              "      <td>b'Russia, Georgia, and NATO: Cold War Two'</td>\n",
              "      <td>b'Remember that adorable 62-year-old who led y...</td>\n",
              "      <td>b'War in Georgia: The Israeli connection'</td>\n",
              "      <td>b'All signs point to the US encouraging Georgi...</td>\n",
              "      <td>b'Christopher King argues that the US and NATO...</td>\n",
              "      <td>b'America: The New Mexico?'</td>\n",
              "      <td>b\"BBC NEWS | Asia-Pacific | Extinction 'by man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-08-13</td>\n",
              "      <td>0</td>\n",
              "      <td>b' U.S. refuses Israel weapons to attack Iran:...</td>\n",
              "      <td>b\"When the president ordered to attack Tskhinv...</td>\n",
              "      <td>b' Israel clears troops who killed Reuters cam...</td>\n",
              "      <td>b'Britain\\'s policy of being tough on drugs is...</td>\n",
              "      <td>b'Body of 14 year old found in trunk; Latest (...</td>\n",
              "      <td>b'China has moved 10 *million* quake survivors...</td>\n",
              "      <td>b\"Bush announces Operation Get All Up In Russi...</td>\n",
              "      <td>b'Russian forces sink Georgian ships '</td>\n",
              "      <td>...</td>\n",
              "      <td>b'Elephants extinct by 2020?'</td>\n",
              "      <td>b'US humanitarian missions soon in Georgia - i...</td>\n",
              "      <td>b\"Georgia's DDOS came from US sources\"</td>\n",
              "      <td>b'Russian convoy heads into Georgia, violating...</td>\n",
              "      <td>b'Israeli defence minister: US against strike ...</td>\n",
              "      <td>b'Gorbachev: We Had No Choice'</td>\n",
              "      <td>b'Witness: Russian forces head towards Tbilisi...</td>\n",
              "      <td>b' Quarter of Russians blame U.S. for conflict...</td>\n",
              "      <td>b'Georgian president  says US military will ta...</td>\n",
              "      <td>b'2006: Nobel laureate Aleksander Solzhenitsyn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-08-14</td>\n",
              "      <td>1</td>\n",
              "      <td>b'All the experts admit that we should legalis...</td>\n",
              "      <td>b'War in South Osetia - 89 pictures made by a ...</td>\n",
              "      <td>b'Swedish wrestler Ara Abrahamian throws away ...</td>\n",
              "      <td>b'Russia exaggerated the death toll in South O...</td>\n",
              "      <td>b'Missile That Killed 9 Inside Pakistan May Ha...</td>\n",
              "      <td>b\"Rushdie Condemns Random House's Refusal to P...</td>\n",
              "      <td>b'Poland and US agree to missle defense deal. ...</td>\n",
              "      <td>b'Will the Russians conquer Tblisi? Bet on it,...</td>\n",
              "      <td>...</td>\n",
              "      <td>b'Bank analyst forecast Georgian crisis 2 days...</td>\n",
              "      <td>b\"Georgia confict could set back Russia's US r...</td>\n",
              "      <td>b'War in the Caucasus is as much the product o...</td>\n",
              "      <td>b'\"Non-media\" photos of South Ossetia/Georgia ...</td>\n",
              "      <td>b'Georgian TV reporter shot by Russian sniper ...</td>\n",
              "      <td>b'Saudi Arabia: Mother moves to block child ma...</td>\n",
              "      <td>b'Taliban wages war on humanitarian aid workers'</td>\n",
              "      <td>b'Russia: World  \"can forget about\" Georgia\\'s...</td>\n",
              "      <td>b'Darfur rebels accuse Sudan of mounting major...</td>\n",
              "      <td>b'Philippines : Peace Advocate say Muslims nee...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 27 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fcb6631a-7be3-4ff1-9cf3-1ff930729d64')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fcb6631a-7be3-4ff1-9cf3-1ff930729d64 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fcb6631a-7be3-4ff1-9cf3-1ff930729d64');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d7efba64-fc3f-4063-b845-2867f07281c1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d7efba64-fc3f-4063-b845-2867f07281c1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d7efba64-fc3f-4063-b845-2867f07281c1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-Test Split\n",
        "\n",
        "In time series problems, old data are used to predict later data. Therefore, we set test data to be the latest data."
      ],
      "metadata": {
        "id": "TpYU4o-DfjLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = df[df['Date'] < '20150101']\n",
        "test = df[df['Date'] > '20141231']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T11:10:55.882721Z",
          "iopub.execute_input": "2022-04-07T11:10:55.883125Z",
          "iopub.status.idle": "2022-04-07T11:10:55.901287Z",
          "shell.execute_reply.started": "2022-04-07T11:10:55.883081Z",
          "shell.execute_reply": "2022-04-07T11:10:55.900171Z"
        },
        "trusted": true,
        "id": "fdZyH7OBfjLV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Transformation\n",
        "From inpecting the data, we learned that there are a lot of punctuations which will not contribute to understanding the sentiment in the message. We can use \"replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\" to replace everything apart from a-z and A-Z with blank space and store in-place in the \"data\" variable.\n",
        "\n",
        "In other NLP applications, stop words like articles, prepositions, pronouns, conjunction are removed to give more emphasis to the important words. It was not done here as news headlines are generally condensed and every word have particular meaning.\n",
        "\n",
        "Regular expressions (regex) are essentially text patterns that you can use to automate searching through and replacing elements within strings of text. This can make cleaning and working with text-based data sets much easier, saving you the trouble of having to search through mountains of text by hand."
      ],
      "metadata": {
        "id": "lRgDnKaKfjLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing punctuations\n",
        "data=train.iloc[:,2:27]\n",
        "data.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n",
        "\n",
        "# Renaming column names for ease of access\n",
        "list1= [i for i in range(25)]\n",
        "new_Index=[str(i) for i in list1]\n",
        "data.columns= new_Index\n",
        "data.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T11:37:02.068416Z",
          "iopub.execute_input": "2022-04-07T11:37:02.069683Z",
          "iopub.status.idle": "2022-04-07T11:37:02.664629Z",
          "shell.execute_reply.started": "2022-04-07T11:37:02.069604Z",
          "shell.execute_reply": "2022-04-07T11:37:02.663959Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "pmXmQg5dfjLV",
        "outputId": "6bd8a0a3-2aba-4c5c-9939-237e2523b306"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   0  \\\n",
              "0  b Georgia  downs two Russian warplanes  as cou...   \n",
              "1  b Why wont America and Nato help us  If they w...   \n",
              "2  b Remember that adorable   year old who sang a...   \n",
              "3  b  U S  refuses Israel weapons to attack Iran ...   \n",
              "4  b All the experts admit that we should legalis...   \n",
              "\n",
              "                                                   1  \\\n",
              "0            b BREAKING  Musharraf to be impeached     \n",
              "1        b Bush puts foot down on Georgian conflict    \n",
              "2                 b Russia  ends Georgia operation     \n",
              "3  b When the president ordered to attack Tskhinv...   \n",
              "4  b War in South Osetia      pictures made by a ...   \n",
              "\n",
              "                                                   2  \\\n",
              "0  b Russia Today  Columns of troops roll into So...   \n",
              "1  b Jewish Georgian minister  Thanks to Israeli ...   \n",
              "2  b  If we had no sexual harassment we would hav...   \n",
              "3  b  Israel clears troops who killed Reuters cam...   \n",
              "4  b Swedish wrestler Ara Abrahamian throws away ...   \n",
              "\n",
              "                                                   3  \\\n",
              "0  b Russian tanks are moving towards the capital...   \n",
              "1  b Georgian army flees in disarray as Russians ...   \n",
              "2  b Al Qa eda is losing support in Iraq because ...   \n",
              "3  b Britain  s policy of being tough on drugs is...   \n",
              "4  b Russia exaggerated the death toll in South O...   \n",
              "\n",
              "                                                   4  \\\n",
              "0  b Afghan children raped with  impunity   U N  ...   \n",
              "1      b Olympic opening ceremony fireworks  faked     \n",
              "2  b Ceasefire in Georgia  Putin Outmaneuvers the...   \n",
              "3  b Body of    year old found in trunk  Latest  ...   \n",
              "4  b Missile That Killed   Inside Pakistan May Ha...   \n",
              "\n",
              "                                                   5  \\\n",
              "0  b     Russian tanks have entered South Ossetia...   \n",
              "1  b What were the Mossad with fraudulent New Zea...   \n",
              "2  b Why Microsoft and Intel tried to kill the XO...   \n",
              "3  b China has moved     million  quake survivors...   \n",
              "4  b Rushdie Condemns Random House s Refusal to P...   \n",
              "\n",
              "                                                   6  \\\n",
              "0  b Breaking  Georgia invades South Ossetia  Rus...   \n",
              "1  b Russia angered by Israeli military sale to G...   \n",
              "2  b Stratfor  The Russo Georgian War and the Bal...   \n",
              "3  b Bush announces Operation Get All Up In Russi...   \n",
              "4  b Poland and US agree to missle defense deal  ...   \n",
              "\n",
              "                                                   7  \\\n",
              "0  b The  enemy combatent  trials are nothing but...   \n",
              "1  b An American citizen living in S Ossetia blam...   \n",
              "2  b I m Trying to Get a Sense of This Whole Geor...   \n",
              "3             b Russian forces sink Georgian ships     \n",
              "4  b Will the Russians conquer Tblisi  Bet on it ...   \n",
              "\n",
              "                                                   8  \\\n",
              "0  b Georgian troops retreat from S  Osettain cap...   \n",
              "1  b Welcome To World War IV  Now In High Definit...   \n",
              "2  b The US military was surprised by the timing ...   \n",
              "3  b The commander of a Navy air reconnaissance s...   \n",
              "4  b Russia exaggerating South Ossetian death tol...   \n",
              "\n",
              "                                                   9  ...  \\\n",
              "0  b Did the U S  Prep Georgia for War with Russia    ...   \n",
              "1  b Georgia s move  a mistake of monumental prop...  ...   \n",
              "2    b U S  Beats War Drum as Iran Dumps the Dollar   ...   \n",
              "3  b     of CNN readers  Russia s actions in Geor...  ...   \n",
              "4  b  Musharraf expected to resign rather than fa...  ...   \n",
              "\n",
              "                                                  15  \\\n",
              "0  b Georgia Invades South Ossetia   if Russia ge...   \n",
              "1  b Israel and the US behind the Georgian aggres...   \n",
              "2  b U S  troops still in Georgia  did you know t...   \n",
              "3                      b Elephants extinct by          \n",
              "4  b Bank analyst forecast Georgian crisis   days...   \n",
              "\n",
              "                                                  16  \\\n",
              "0                b Al Qaeda Faces Islamist Backlash    \n",
              "1  b  Do not believe TV  neither Russian nor Geor...   \n",
              "2       b Why Russias response to Georgia was right    \n",
              "3  b US humanitarian missions soon in Georgia   i...   \n",
              "4  b Georgia confict could set back Russia s US r...   \n",
              "\n",
              "                                                  17  \\\n",
              "0  b Condoleezza Rice   The US would not act to p...   \n",
              "1  b Riots are still going on in Montreal  Canada...   \n",
              "2  b Gorbachev accuses U S  of making a  serious ...   \n",
              "3             b Georgia s DDOS came from US sources    \n",
              "4  b War in the Caucasus is as much the product o...   \n",
              "\n",
              "                                                  18  \\\n",
              "0  b This is a busy day   The European Union has ...   \n",
              "1    b China to overtake US as largest manufacturer    \n",
              "2         b Russia  Georgia  and NATO  Cold War Two    \n",
              "3  b Russian convoy heads into Georgia  violating...   \n",
              "4  b  Non media  photos of South Ossetia Georgia ...   \n",
              "\n",
              "                                                  19  \\\n",
              "0  b Georgia will withdraw       soldiers from Ir...   \n",
              "1                     b War in South Ossetia  PICS     \n",
              "2  b Remember that adorable    year old who led y...   \n",
              "3  b Israeli defence minister  US against strike ...   \n",
              "4  b Georgian TV reporter shot by Russian sniper ...   \n",
              "\n",
              "                                                  20  \\\n",
              "0  b Why the Pentagon Thinks Attacking Iran is a ...   \n",
              "1  b Israeli Physicians Group Condemns State Tort...   \n",
              "2          b War in Georgia  The Israeli connection    \n",
              "3                     b Gorbachev  We Had No Choice    \n",
              "4  b Saudi Arabia  Mother moves to block child ma...   \n",
              "\n",
              "                                                  21  \\\n",
              "0  b Caucasus in crisis  Georgia invades South Os...   \n",
              "1  b  Russia has just beaten the United States ov...   \n",
              "2  b All signs point to the US encouraging Georgi...   \n",
              "3  b Witness  Russian forces head towards Tbilisi...   \n",
              "4   b Taliban wages war on humanitarian aid workers    \n",
              "\n",
              "                                                  22  \\\n",
              "0  b Indian shoe manufactory    And again in a se...   \n",
              "1  b Perhaps  the  question about the Georgia   R...   \n",
              "2  b Christopher King argues that the US and NATO...   \n",
              "3  b  Quarter of Russians blame U S  for conflict...   \n",
              "4  b Russia  World   can forget about  Georgia  s...   \n",
              "\n",
              "                                                  23  \\\n",
              "0  b Visitors Suffering from Mental Illnesses Ban...   \n",
              "1                 b Russia is so much better at war    \n",
              "2                        b America  The New Mexico     \n",
              "3  b Georgian president  says US military will ta...   \n",
              "4  b Darfur rebels accuse Sudan of mounting major...   \n",
              "\n",
              "                                                  24  \n",
              "0           b No Help for Mexico s Kidnapping Surge   \n",
              "1  b So this is what it s come to  trading sex fo...  \n",
              "2  b BBC NEWS   Asia Pacific   Extinction  by man...  \n",
              "3  b       Nobel laureate Aleksander Solzhenitsyn...  \n",
              "4  b Philippines   Peace Advocate say Muslims nee...  \n",
              "\n",
              "[5 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-efd9c18c-f1ac-4732-84ec-0aae0c1cc17e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b Georgia  downs two Russian warplanes  as cou...</td>\n",
              "      <td>b BREAKING  Musharraf to be impeached</td>\n",
              "      <td>b Russia Today  Columns of troops roll into So...</td>\n",
              "      <td>b Russian tanks are moving towards the capital...</td>\n",
              "      <td>b Afghan children raped with  impunity   U N  ...</td>\n",
              "      <td>b     Russian tanks have entered South Ossetia...</td>\n",
              "      <td>b Breaking  Georgia invades South Ossetia  Rus...</td>\n",
              "      <td>b The  enemy combatent  trials are nothing but...</td>\n",
              "      <td>b Georgian troops retreat from S  Osettain cap...</td>\n",
              "      <td>b Did the U S  Prep Georgia for War with Russia</td>\n",
              "      <td>...</td>\n",
              "      <td>b Georgia Invades South Ossetia   if Russia ge...</td>\n",
              "      <td>b Al Qaeda Faces Islamist Backlash</td>\n",
              "      <td>b Condoleezza Rice   The US would not act to p...</td>\n",
              "      <td>b This is a busy day   The European Union has ...</td>\n",
              "      <td>b Georgia will withdraw       soldiers from Ir...</td>\n",
              "      <td>b Why the Pentagon Thinks Attacking Iran is a ...</td>\n",
              "      <td>b Caucasus in crisis  Georgia invades South Os...</td>\n",
              "      <td>b Indian shoe manufactory    And again in a se...</td>\n",
              "      <td>b Visitors Suffering from Mental Illnesses Ban...</td>\n",
              "      <td>b No Help for Mexico s Kidnapping Surge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b Why wont America and Nato help us  If they w...</td>\n",
              "      <td>b Bush puts foot down on Georgian conflict</td>\n",
              "      <td>b Jewish Georgian minister  Thanks to Israeli ...</td>\n",
              "      <td>b Georgian army flees in disarray as Russians ...</td>\n",
              "      <td>b Olympic opening ceremony fireworks  faked</td>\n",
              "      <td>b What were the Mossad with fraudulent New Zea...</td>\n",
              "      <td>b Russia angered by Israeli military sale to G...</td>\n",
              "      <td>b An American citizen living in S Ossetia blam...</td>\n",
              "      <td>b Welcome To World War IV  Now In High Definit...</td>\n",
              "      <td>b Georgia s move  a mistake of monumental prop...</td>\n",
              "      <td>...</td>\n",
              "      <td>b Israel and the US behind the Georgian aggres...</td>\n",
              "      <td>b  Do not believe TV  neither Russian nor Geor...</td>\n",
              "      <td>b Riots are still going on in Montreal  Canada...</td>\n",
              "      <td>b China to overtake US as largest manufacturer</td>\n",
              "      <td>b War in South Ossetia  PICS</td>\n",
              "      <td>b Israeli Physicians Group Condemns State Tort...</td>\n",
              "      <td>b  Russia has just beaten the United States ov...</td>\n",
              "      <td>b Perhaps  the  question about the Georgia   R...</td>\n",
              "      <td>b Russia is so much better at war</td>\n",
              "      <td>b So this is what it s come to  trading sex fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b Remember that adorable   year old who sang a...</td>\n",
              "      <td>b Russia  ends Georgia operation</td>\n",
              "      <td>b  If we had no sexual harassment we would hav...</td>\n",
              "      <td>b Al Qa eda is losing support in Iraq because ...</td>\n",
              "      <td>b Ceasefire in Georgia  Putin Outmaneuvers the...</td>\n",
              "      <td>b Why Microsoft and Intel tried to kill the XO...</td>\n",
              "      <td>b Stratfor  The Russo Georgian War and the Bal...</td>\n",
              "      <td>b I m Trying to Get a Sense of This Whole Geor...</td>\n",
              "      <td>b The US military was surprised by the timing ...</td>\n",
              "      <td>b U S  Beats War Drum as Iran Dumps the Dollar</td>\n",
              "      <td>...</td>\n",
              "      <td>b U S  troops still in Georgia  did you know t...</td>\n",
              "      <td>b Why Russias response to Georgia was right</td>\n",
              "      <td>b Gorbachev accuses U S  of making a  serious ...</td>\n",
              "      <td>b Russia  Georgia  and NATO  Cold War Two</td>\n",
              "      <td>b Remember that adorable    year old who led y...</td>\n",
              "      <td>b War in Georgia  The Israeli connection</td>\n",
              "      <td>b All signs point to the US encouraging Georgi...</td>\n",
              "      <td>b Christopher King argues that the US and NATO...</td>\n",
              "      <td>b America  The New Mexico</td>\n",
              "      <td>b BBC NEWS   Asia Pacific   Extinction  by man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b  U S  refuses Israel weapons to attack Iran ...</td>\n",
              "      <td>b When the president ordered to attack Tskhinv...</td>\n",
              "      <td>b  Israel clears troops who killed Reuters cam...</td>\n",
              "      <td>b Britain  s policy of being tough on drugs is...</td>\n",
              "      <td>b Body of    year old found in trunk  Latest  ...</td>\n",
              "      <td>b China has moved     million  quake survivors...</td>\n",
              "      <td>b Bush announces Operation Get All Up In Russi...</td>\n",
              "      <td>b Russian forces sink Georgian ships</td>\n",
              "      <td>b The commander of a Navy air reconnaissance s...</td>\n",
              "      <td>b     of CNN readers  Russia s actions in Geor...</td>\n",
              "      <td>...</td>\n",
              "      <td>b Elephants extinct by</td>\n",
              "      <td>b US humanitarian missions soon in Georgia   i...</td>\n",
              "      <td>b Georgia s DDOS came from US sources</td>\n",
              "      <td>b Russian convoy heads into Georgia  violating...</td>\n",
              "      <td>b Israeli defence minister  US against strike ...</td>\n",
              "      <td>b Gorbachev  We Had No Choice</td>\n",
              "      <td>b Witness  Russian forces head towards Tbilisi...</td>\n",
              "      <td>b  Quarter of Russians blame U S  for conflict...</td>\n",
              "      <td>b Georgian president  says US military will ta...</td>\n",
              "      <td>b       Nobel laureate Aleksander Solzhenitsyn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b All the experts admit that we should legalis...</td>\n",
              "      <td>b War in South Osetia      pictures made by a ...</td>\n",
              "      <td>b Swedish wrestler Ara Abrahamian throws away ...</td>\n",
              "      <td>b Russia exaggerated the death toll in South O...</td>\n",
              "      <td>b Missile That Killed   Inside Pakistan May Ha...</td>\n",
              "      <td>b Rushdie Condemns Random House s Refusal to P...</td>\n",
              "      <td>b Poland and US agree to missle defense deal  ...</td>\n",
              "      <td>b Will the Russians conquer Tblisi  Bet on it ...</td>\n",
              "      <td>b Russia exaggerating South Ossetian death tol...</td>\n",
              "      <td>b  Musharraf expected to resign rather than fa...</td>\n",
              "      <td>...</td>\n",
              "      <td>b Bank analyst forecast Georgian crisis   days...</td>\n",
              "      <td>b Georgia confict could set back Russia s US r...</td>\n",
              "      <td>b War in the Caucasus is as much the product o...</td>\n",
              "      <td>b  Non media  photos of South Ossetia Georgia ...</td>\n",
              "      <td>b Georgian TV reporter shot by Russian sniper ...</td>\n",
              "      <td>b Saudi Arabia  Mother moves to block child ma...</td>\n",
              "      <td>b Taliban wages war on humanitarian aid workers</td>\n",
              "      <td>b Russia  World   can forget about  Georgia  s...</td>\n",
              "      <td>b Darfur rebels accuse Sudan of mounting major...</td>\n",
              "      <td>b Philippines   Peace Advocate say Muslims nee...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 25 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efd9c18c-f1ac-4732-84ec-0aae0c1cc17e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-efd9c18c-f1ac-4732-84ec-0aae0c1cc17e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-efd9c18c-f1ac-4732-84ec-0aae0c1cc17e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-00bc106b-8e53-47bf-8101-90ac26684d22\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-00bc106b-8e53-47bf-8101-90ac26684d22')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-00bc106b-8e53-47bf-8101-90ac26684d22 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the case of letters in the words does not affect its sentiment, we can convert all the characters to lower case."
      ],
      "metadata": {
        "id": "DMfkG1nnfjLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertng headlines to lower case\n",
        "for index in new_Index:\n",
        "    data[index]=data[index].str.lower()\n",
        "data.head(1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T11:37:39.398334Z",
          "iopub.execute_input": "2022-04-07T11:37:39.399547Z",
          "iopub.status.idle": "2022-04-07T11:37:39.464675Z",
          "shell.execute_reply.started": "2022-04-07T11:37:39.399502Z",
          "shell.execute_reply": "2022-04-07T11:37:39.463792Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "JZWgbg5tfjLW",
        "outputId": "d31e68de-f908-42d8-abdf-4db50d7b3e8a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   0  \\\n",
              "0  b georgia  downs two russian warplanes  as cou...   \n",
              "\n",
              "                                         1  \\\n",
              "0  b breaking  musharraf to be impeached     \n",
              "\n",
              "                                                   2  \\\n",
              "0  b russia today  columns of troops roll into so...   \n",
              "\n",
              "                                                   3  \\\n",
              "0  b russian tanks are moving towards the capital...   \n",
              "\n",
              "                                                   4  \\\n",
              "0  b afghan children raped with  impunity   u n  ...   \n",
              "\n",
              "                                                   5  \\\n",
              "0  b     russian tanks have entered south ossetia...   \n",
              "\n",
              "                                                   6  \\\n",
              "0  b breaking  georgia invades south ossetia  rus...   \n",
              "\n",
              "                                                   7  \\\n",
              "0  b the  enemy combatent  trials are nothing but...   \n",
              "\n",
              "                                                   8  \\\n",
              "0  b georgian troops retreat from s  osettain cap...   \n",
              "\n",
              "                                                   9  ...  \\\n",
              "0  b did the u s  prep georgia for war with russia    ...   \n",
              "\n",
              "                                                  15  \\\n",
              "0  b georgia invades south ossetia   if russia ge...   \n",
              "\n",
              "                                    16  \\\n",
              "0  b al qaeda faces islamist backlash    \n",
              "\n",
              "                                                  17  \\\n",
              "0  b condoleezza rice   the us would not act to p...   \n",
              "\n",
              "                                                  18  \\\n",
              "0  b this is a busy day   the european union has ...   \n",
              "\n",
              "                                                  19  \\\n",
              "0  b georgia will withdraw       soldiers from ir...   \n",
              "\n",
              "                                                  20  \\\n",
              "0  b why the pentagon thinks attacking iran is a ...   \n",
              "\n",
              "                                                  21  \\\n",
              "0  b caucasus in crisis  georgia invades south os...   \n",
              "\n",
              "                                                  22  \\\n",
              "0  b indian shoe manufactory    and again in a se...   \n",
              "\n",
              "                                                  23  \\\n",
              "0  b visitors suffering from mental illnesses ban...   \n",
              "\n",
              "                                         24  \n",
              "0  b no help for mexico s kidnapping surge   \n",
              "\n",
              "[1 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-34f69e14-d239-45ac-accd-fae3b40adcb7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b georgia  downs two russian warplanes  as cou...</td>\n",
              "      <td>b breaking  musharraf to be impeached</td>\n",
              "      <td>b russia today  columns of troops roll into so...</td>\n",
              "      <td>b russian tanks are moving towards the capital...</td>\n",
              "      <td>b afghan children raped with  impunity   u n  ...</td>\n",
              "      <td>b     russian tanks have entered south ossetia...</td>\n",
              "      <td>b breaking  georgia invades south ossetia  rus...</td>\n",
              "      <td>b the  enemy combatent  trials are nothing but...</td>\n",
              "      <td>b georgian troops retreat from s  osettain cap...</td>\n",
              "      <td>b did the u s  prep georgia for war with russia</td>\n",
              "      <td>...</td>\n",
              "      <td>b georgia invades south ossetia   if russia ge...</td>\n",
              "      <td>b al qaeda faces islamist backlash</td>\n",
              "      <td>b condoleezza rice   the us would not act to p...</td>\n",
              "      <td>b this is a busy day   the european union has ...</td>\n",
              "      <td>b georgia will withdraw       soldiers from ir...</td>\n",
              "      <td>b why the pentagon thinks attacking iran is a ...</td>\n",
              "      <td>b caucasus in crisis  georgia invades south os...</td>\n",
              "      <td>b indian shoe manufactory    and again in a se...</td>\n",
              "      <td>b visitors suffering from mental illnesses ban...</td>\n",
              "      <td>b no help for mexico s kidnapping surge</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 25 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34f69e14-d239-45ac-accd-fae3b40adcb7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-34f69e14-d239-45ac-accd-fae3b40adcb7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-34f69e14-d239-45ac-accd-fae3b40adcb7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we are going to combine the headings so that we can convert it to a vector so that we can apply the it to the NLP (NLP models usually involve working with vectors).\n",
        "\n",
        "We will join every headings a link them with blank spaces. \".iloc\" is Purely integer-location based indexing for selection by position."
      ],
      "metadata": {
        "id": "2ohzOSRzfjLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headlines = []\n",
        "for row in range(0,len(data.index)):\n",
        "    headlines.append(' '.join(str(x) for x in data.iloc[row,0:25]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T11:38:06.653162Z",
          "iopub.execute_input": "2022-04-07T11:38:06.653521Z",
          "iopub.status.idle": "2022-04-07T11:38:06.93011Z",
          "shell.execute_reply.started": "2022-04-07T11:38:06.653453Z",
          "shell.execute_reply": "2022-04-07T11:38:06.929217Z"
        },
        "trusted": true,
        "id": "POgEEW6EfjLW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the sentences are included in a list as shown below."
      ],
      "metadata": {
        "id": "evnFU3J0fjLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headlines[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T11:38:19.904041Z",
          "iopub.execute_input": "2022-04-07T11:38:19.904555Z",
          "iopub.status.idle": "2022-04-07T11:38:19.911053Z",
          "shell.execute_reply.started": "2022-04-07T11:38:19.904489Z",
          "shell.execute_reply": "2022-04-07T11:38:19.909992Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "tIQihDjmfjLW",
        "outputId": "177fd46e-a070-4e72-e13b-56bc350c2c4f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'b georgia  downs two russian warplanes  as countries move to brink of war  b breaking  musharraf to be impeached   b russia today  columns of troops roll into south ossetia  footage from fighting  youtube   b russian tanks are moving towards the capital of south ossetia  which has reportedly been completely destroyed by georgian artillery fire  b afghan children raped with  impunity   u n  official says   this is sick  a three year old was raped and they do nothing  b     russian tanks have entered south ossetia whilst georgia shoots down two russian jets   b breaking  georgia invades south ossetia  russia warned it would intervene on so s side  b the  enemy combatent  trials are nothing but a sham  salim haman has been sentenced to       years  but will be kept longer anyway just because they feel like it   b georgian troops retreat from s  osettain capital  presumably leaving several hundred people killed   video   b did the u s  prep georgia for war with russia   b rice gives green light for israel to attack iran  says u s  has no veto over israeli military ops  b announcing class action lawsuit on behalf of american public against the fbi  b so   russia and georgia are at war and the nyt s top story is opening ceremonies of the olympics   what a fucking disgrace and yet further proof of the decline of journalism   b china tells bush to stay out of other countries  affairs  b did world war iii start today   b georgia invades south ossetia   if russia gets involved  will nato absorb georgia and unleash a full scale war   b al qaeda faces islamist backlash  b condoleezza rice   the us would not act to prevent an israeli strike on iran   israeli defense minister ehud barak   israel is prepared for uncompromising victory in the case of military hostilities    b this is a busy day   the european union has approved new sanctions against iran in protest at its nuclear programme   b georgia will withdraw       soldiers from iraq to help fight off russian forces in georgia s breakaway region of south ossetia  b why the pentagon thinks attacking iran is a bad idea   us news  amp  world report  b caucasus in crisis  georgia invades south ossetia  b indian shoe manufactory    and again in a series of  you do not like your work    b visitors suffering from mental illnesses banned from olympics  b no help for mexico s kidnapping surge '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count Vectorizer\n",
        "Count Vectorizer will represent words by numbers so that they can be processed by algorithms. This is done by making each unique word a column and count the number of times each word appear and then make it the row values. An example is given below.\n",
        "\n",
        "![](https://www.educative.io/api/edpresso/shot/5197621598617600/image/6596233398321152)\n",
        "\n",
        "ngram is set as 2,2 so that pairs of words appearing together are used as features (columns)\n",
        "If we set (1, 1) means only unigrams(sngle words), (1, 2) means unigrams and bigrams(2 words), and (2, 2) means only bigrams."
      ],
      "metadata": {
        "id": "0nhgKF09fjLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## implement BAG OF WORDS\n",
        "countvector=CountVectorizer(ngram_range=(2,2))\n",
        "traindataset=countvector.fit_transform(headlines)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T11:38:57.84717Z",
          "iopub.execute_input": "2022-04-07T11:38:57.847519Z",
          "iopub.status.idle": "2022-04-07T11:39:00.95594Z",
          "shell.execute_reply.started": "2022-04-07T11:38:57.847453Z",
          "shell.execute_reply": "2022-04-07T11:39:00.955244Z"
        },
        "trusted": true,
        "id": "PI874Y6wfjLW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Preprocessing functions\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Replace non-alphabet characters with spaces\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "def stem_and_lemmatize(words):\n",
        "    stemmed = [ps.stem(word) for word in words]\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in stemmed]\n",
        "    return lemmatized\n",
        "\n",
        "def preprocess_headline(headline):\n",
        "    cleaned = clean_text(headline)\n",
        "    tokens = tokenize_and_remove_stopwords(cleaned)\n",
        "    normalized = stem_and_lemmatize(tokens)\n",
        "    return \" \".join(normalized)\n",
        "# Encoding are in general UTF-8(default setting), Latin-1 (also known as ISO-8859-1) or Windows-1251\n",
        "df=pd.read_csv('Combined_News_DJIA.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "train = df[df['Date'] < '20150101']\n",
        "test = df[df['Date'] > '20141231']\n",
        "\n",
        "# Preprocess the training headlines\n",
        "data_train = train.iloc[:, 2:27]\n",
        "data_train.columns = [str(i) for i in range(25)]\n",
        "\n",
        "train_headlines = []\n",
        "for row in range(0, len(data_train.index)):\n",
        "    combined_headlines = ' '.join(str(x) for x in data_train.iloc[row, 0:25])\n",
        "    preprocessed_headline = preprocess_headline(combined_headlines)\n",
        "    train_headlines.append(preprocessed_headline)\n",
        "\n",
        "\n",
        "\n",
        "# Preprocess the testing headlines\n",
        "data_test = test.iloc[:, 2:27]\n",
        "data_test.columns = [str(i) for i in range(25)]\n",
        "test_headlines = []\n",
        "for row in range(0, len(data_test.index)):\n",
        "    combined_headlines = ' '.join(str(x) for x in data_test.iloc[row, 0:25])\n",
        "    preprocessed_headline = preprocess_headline(combined_headlines)\n",
        "    test_headlines.append(preprocessed_headline)\n",
        "\n",
        "# Create the target labels\n",
        "y_train = train['Label'].values\n",
        "y_test = test['Label'].values\n",
        "\n",
        "# Create Bag of Words representation with bigrams\n",
        "countvector = CountVectorizer(ngram_range=(2, 2))\n",
        "X_train = countvector.fit_transform(train_headlines)\n",
        "X_test = countvector.transform(test_headlines)\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)\n",
        "\n",
        "# Print a sample of the preprocessed headlines and their Bag of Words representation\n",
        "print(\"\\nSample of preprocessed headlines:\")\n",
        "print(train_headlines[:2])\n",
        "print(\"\\nBag of Words representation (feature names):\")\n",
        "print(countvector.get_feature_names_out()[:10])  # Displaying first 10 feature names for brevity\n",
        "print(\"\\nBag of Words representation (dense matrix):\")\n",
        "print(X_train.toarray()[:2])  # Displaying first 2 rows for brevity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oA8rcrKwT_o",
        "outputId": "677488f8-d182-4c76-aeec-760c0c98692f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (1863, 374550)\n",
            "Testing set shape: (378, 374550)\n",
            "\n",
            "Sample of preprocessed headlines:\n",
            "['b georgia down two russian warplan countri move brink war b break musharraf impeach b russia today column troop roll south ossetia footag fight youtub b russian tank move toward capit south ossetia reportedli complet destroy georgian artilleri fire b afghan child rape impun u n offici say sick three year old rape noth b russian tank enter south ossetia whilst georgia shoot two russian jet b break georgia invad south ossetia russia warn would interven side b enemi combat trial noth sham salim haman sentenc year kept longer anyway feel like b georgian troop retreat osettain capit presum leav sever hundr peopl kill video b u prep georgia war russia b rice give green light israel attack iran say u veto isra militari op b announc class action lawsuit behalf american public fbi b russia georgia war nyt top stori open ceremoni olymp fuck disgrac yet proof declin journal b china tell bush stay countri affair b world war iii start today b georgia invad south ossetia russia get involv nato absorb georgia unleash full scale war b al qaeda face islamist backlash b condoleezza rice u would act prevent isra strike iran isra defens minist ehud barak israel prepar uncompromis victori case militari hostil b busi day european union approv new sanction iran protest nuclear programm b georgia withdraw soldier iraq help fight russian forc georgia breakaway region south ossetia b pentagon think attack iran bad idea u news amp world report b caucasu crisi georgia invad south ossetia b indian shoe manufactori seri like work b visitor suffer mental ill ban olymp b help mexico kidnap surg', 'b wont america nato help u wont help u help iraq b bush put foot georgian conflict b jewish georgian minist thank isra train fend russia b georgian armi flee disarray russian advanc gori abandon russia without shot fire b olymp open ceremoni firework fake b mossad fraudul new zealand passport iraq b russia anger isra militari sale georgia b american citizen live ossetia blame u georgian leader genocid innoc peopl b welcom world war iv high definit b georgia move mistak monument proport b russia press deeper georgia u say regim chang goal b abhinav bindra win first ever individu olymp gold medal india b u ship head arctic defin territori b driver jerusalem taxi station threaten quit rather work new bos arab b french team stun phelp x relay team b israel u behind georgian aggress b believ tv neither russian georgian much victim b riot still go montreal canada polic murder boy saturday b china overtak u largest manufactur b war south ossetia pic b isra physician group condemn state tortur b russia beaten unit state head peak oil b perhap question georgia russia conflict b russia much better war b come trade sex food']\n",
            "\n",
            "Bag of Words representation (feature names):\n",
            "['aa pakistan' 'aaa aab' 'aaa credit' 'aaa rate' 'aaa seal' 'aaaw nice'\n",
            " 'aab world' 'aabo abl' 'aadmi parti' 'aafia disappear']\n",
            "\n",
            "Bag of Words representation (dense matrix):\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model - RandomForest\n",
        "RandomForest is a flexible, easy to use machine learning algorithm that produces, even without hyper-parameter tuning, a great result most of the time. It is also one of the most used algorithms, because of its simplicity and diversity. It works by creating a set of decision trees from a randomly selected subset of the training set. It is basically a set of decision trees (DT) from a randomly selected subset of the training set and then It collects the votes from different decision trees to decide the final prediction.\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Random_forest_diagram_complete.png/220px-Random_forest_diagram_complete.png)\n",
        "\n",
        "The parameters for our classifier are as follow:\n",
        "* n_estimators : This is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower.\n",
        "* crierion : This is how each node of the tree is selected. There are 2 options:\n",
        "  * gini: much faster\n",
        "  * entropy: obtained results are slightly better\n",
        "\n",
        "RandomForest Classifier is used here because we want sboth speed and accuracy.\n",
        "\n",
        "For details on selecting ML models, please refer to the below cheatsheet.\n",
        "![](https://miro.medium.com/max/1400/1*fXm06pz5UGX_N_Y0j9X2IA.png)"
      ],
      "metadata": {
        "id": "kSyTk_ZifjLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement RandomForest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "randomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\n",
        "randomclassifier.fit(X_train,y_train)# implement RandomForest Classifier"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T11:39:08.572519Z",
          "iopub.execute_input": "2022-04-07T11:39:08.573179Z",
          "iopub.status.idle": "2022-04-07T11:39:24.358934Z",
          "shell.execute_reply.started": "2022-04-07T11:39:08.573139Z",
          "shell.execute_reply": "2022-04-07T11:39:24.357848Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "CGeb68x4fjLW",
        "outputId": "1133526e-bd57-48d5-ba12-49181688ec55"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(criterion='entropy', n_estimators=200)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_estimators=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_estimators=200)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = randomclassifier.predict(X_test)"
      ],
      "metadata": {
        "id": "KDjYnbU7xTCg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "matrix=confusion_matrix(y_test,predictions)\n",
        "print(matrix)\n",
        "score=accuracy_score(y_test,predictions)\n",
        "print(score)\n",
        "report=classification_report(y_test,predictions)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB-mL6kXxYuR",
        "outputId": "fb416168-fee5-4a39-b02a-02c4048a5072"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[130  56]\n",
            " [  0 192]]\n",
            "0.8518518518518519\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.70      0.82       186\n",
            "           1       0.77      1.00      0.87       192\n",
            "\n",
            "    accuracy                           0.85       378\n",
            "   macro avg       0.89      0.85      0.85       378\n",
            "weighted avg       0.89      0.85      0.85       378\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same feature transformations are done for the test set:\n"
      ],
      "metadata": {
        "id": "5tl11dLvfjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We have completed oour first prediction. Let's see how it performed.\n",
        "\n",
        "# Evaluation for Random Forest + Count Vectorizer\n",
        "\n",
        "First we will take a look at its confusion matrix\n",
        "\n",
        "![](http://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix_files/confusion_matrix_1.png)\n",
        "\n",
        "Then let's take a look at the accuracy score and a detailed report.\n",
        "\n",
        "Support is the number of actual occurrences of the class in the specified datase. We can see if the samples are balanced or not.\n",
        "\n",
        "The other terms related are defined as:\n",
        "\n",
        "![](https://i.ytimg.com/vi/ji48Lz6amMc/maxresdefault.jpg)\n",
        "\n"
      ],
      "metadata": {
        "id": "5Px6UhVBfjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the similar supports across two classes, we see that the dataset is balanced.\n",
        "The Accuracy is decent as well.\n",
        "\n",
        "In CountVectorizer we only count the number of times a word appears in the document which results in biasing in favour of most frequent words. this ends up in ignoring rare words which could have helped is in processing our data more efficiently.\n",
        "\n",
        "To overcome this , we use TfidfVectorizer ."
      ],
      "metadata": {
        "id": "4fFRrLjIfjLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF\n",
        "TF-IDF is an abbreviation for Term Frequency Inverse Document Frequency. It measures the relative importance of words by comparing the no. of times a word appear in a sample with the number of samples with that word.\n",
        "\n",
        "This time the vectors are made up of the TF-IDFs of the words in the headings.\n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*qQgnyPLDIkUmeZKN2_ZWbQ.png)"
      ],
      "metadata": {
        "id": "6vI6i8t9fjLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## implement BAG OF WORDS\n",
        "tfidfvector=TfidfVectorizer(ngram_range=(2,2))\n",
        "traindataset=tfidfvector.fit_transform(train_headlines)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T13:33:01.795378Z",
          "iopub.execute_input": "2022-04-07T13:33:01.795684Z",
          "iopub.status.idle": "2022-04-07T13:33:05.139054Z",
          "shell.execute_reply.started": "2022-04-07T13:33:01.795656Z",
          "shell.execute_reply": "2022-04-07T13:33:05.13802Z"
        },
        "trusted": true,
        "id": "7Xul7ekTfjLX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implement RandomForest Classifier\n",
        "randomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\n",
        "randomclassifier.fit(traindataset,y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T13:33:07.731679Z",
          "iopub.execute_input": "2022-04-07T13:33:07.732266Z",
          "iopub.status.idle": "2022-04-07T13:33:24.799294Z",
          "shell.execute_reply.started": "2022-04-07T13:33:07.73223Z",
          "shell.execute_reply": "2022-04-07T13:33:24.798377Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "GAbMfziQfjLX",
        "outputId": "dad27350-e9e8-4617-fbe5-529a80d6b5a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(criterion='entropy', n_estimators=200)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_estimators=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_estimators=200)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation for Random Forest + TF-IDF"
      ],
      "metadata": {
        "id": "xkDvyoB_fjLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = tfidfvector.transform(test_headlines)\n",
        "predictions = randomclassifier.predict(test_dataset)\n",
        "matrix=confusion_matrix(y_test,predictions)\n",
        "print(matrix)\n",
        "score=accuracy_score(y_test,predictions)\n",
        "print(score)\n",
        "report=classification_report(y_test,predictions)\n",
        "print(report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T13:33:34.14494Z",
          "iopub.execute_input": "2022-04-07T13:33:34.145276Z",
          "iopub.status.idle": "2022-04-07T13:33:34.16479Z",
          "shell.execute_reply.started": "2022-04-07T13:33:34.145241Z",
          "shell.execute_reply": "2022-04-07T13:33:34.163974Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4P0wtOXfjLX",
        "outputId": "f92717bb-0be2-4c90-a04f-cea0d5831433"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[136  50]\n",
            " [ 19 173]]\n",
            "0.8174603174603174\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.73      0.80       186\n",
            "           1       0.78      0.90      0.83       192\n",
            "\n",
            "    accuracy                           0.82       378\n",
            "   macro avg       0.83      0.82      0.82       378\n",
            "weighted avg       0.83      0.82      0.82       378\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model - Naive Bayes\n",
        "The accuracy is slightly lower than before. This may be due to the fact that common words (words which will appear in multiple documents) are actually helpful in distinguishing between classes in this case.\n",
        "\n",
        "Next, we are going to use Naive Bayes Classifier.\n",
        "\n",
        "Naive Bayes is another algorithm based on the Bayes Theorem for calculating probabilities and conditional probabilities. It can be extremely fast relative to other classification algorithms.\n",
        "\n",
        "![](https://miro.medium.com/max/1200/1*39U1Ln3tSdFqsfQy6ndxOA.png)\n",
        "For more detail on NB Classifier, check out the tutorial below:\n",
        "https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/"
      ],
      "metadata": {
        "id": "956bK5ytfjLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive=MultinomialNB()\n",
        "naive.fit(traindataset,y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T13:56:07.135812Z",
          "iopub.execute_input": "2022-04-07T13:56:07.136801Z",
          "iopub.status.idle": "2022-04-07T13:56:07.214101Z",
          "shell.execute_reply.started": "2022-04-07T13:56:07.136757Z",
          "shell.execute_reply": "2022-04-07T13:56:07.213149Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "39eqb6lLfjLX",
        "outputId": "f239da36-d897-424d-f533-05b9fcadb139"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation for NB + TF-IDF"
      ],
      "metadata": {
        "id": "3-i_6jjXfjLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = naive.predict(test_dataset)\n",
        "matrix=confusion_matrix(y_test,predictions)\n",
        "print(matrix)\n",
        "score=accuracy_score(y_test,predictions)\n",
        "print(score)\n",
        "report=classification_report(y_test,predictions)\n",
        "print(report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-07T13:57:39.334811Z",
          "iopub.execute_input": "2022-04-07T13:57:39.335168Z",
          "iopub.status.idle": "2022-04-07T13:57:39.354727Z",
          "shell.execute_reply.started": "2022-04-07T13:57:39.335133Z",
          "shell.execute_reply": "2022-04-07T13:57:39.353565Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj4jv5KSfjLX",
        "outputId": "d76c7230-83e8-4706-905c-a2b126e60242"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[131  55]\n",
            " [  2 190]]\n",
            "0.8492063492063492\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.70      0.82       186\n",
            "           1       0.78      0.99      0.87       192\n",
            "\n",
            "    accuracy                           0.85       378\n",
            "   macro avg       0.88      0.85      0.85       378\n",
            "weighted avg       0.88      0.85      0.85       378\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "This time we get a better result with 0 false negative. This shows that although Naive Bayes is generally considered the \"quick fix\", it may produce **equal or even better results** in many cases, especially in **textual applications**.\n",
        "\n",
        "These results also showed that we can produce **decent results on NLP even without the use of Deep Learning** techniques, which is very encouraging for a new Data Scientist.\n",
        "\n",
        "# Next Steps\n",
        "\n",
        "1. It will be intersting to apply LSTM and see if deep learning can produce better result than Naive Bayes.\n",
        "2. Writing an User Interface to make the project end-to-end."
      ],
      "metadata": {
        "id": "uKVNyxHGfjLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, GRU\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_headlines)\n",
        "sequences = tokenizer.texts_to_sequences(train_headlines)\n",
        "word_index = tokenizer.word_index\n",
        "MAX_SEQUENCE_LENGTH = max(len(seq) for seq in sequences)\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Sample target labels (replace with your actual labels)\n",
        "y_train = train['Label'].values\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained Word2Vec embeddings\n",
        "word2vec = api.load(\"word2vec-google-news-300\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lpyL6zFgR2e",
        "outputId": "7d121618-1eda-4452-fe46-ed761e342fa9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = word2vec.vector_size\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec:\n",
        "        embedding_matrix[i] = word2vec[word]\n",
        "\n",
        "def stack_model(layer_name, epochs=50):\n",
        "    SEED = 42\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(word_index) + 1,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=MAX_SEQUENCE_LENGTH,\n",
        "                        trainable=False))\n",
        "    model.add(LSTM(128, return_sequences=True))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=\"adam\", metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, callbacks=[early_stopping])\n",
        "    return model\n",
        "\n",
        "# Example usage with LSTM layers\n",
        "model = stack_model(LSTM, epochs=50)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "cBgYkIqsgeUR",
        "outputId": "f1eb03a9-1089-4c15-d36f-a196a6a4e9b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │       \u001b[38;5;34m6,314,100\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,314,100</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,314,100\u001b[0m (24.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,314,100</span> (24.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m6,314,100\u001b[0m (24.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,314,100</span> (24.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.4622 - loss: 0.7016 - val_accuracy: 0.5871 - val_loss: 0.6826\n",
            "Epoch 2/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.5382 - loss: 0.6918 - val_accuracy: 0.5791 - val_loss: 0.6846\n",
            "Epoch 3/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.5553 - loss: 0.6783 - val_accuracy: 0.5871 - val_loss: 0.6833\n",
            "Epoch 4/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.5868 - loss: 0.6635 - val_accuracy: 0.5389 - val_loss: 0.7110\n",
            "Epoch 5/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.6431 - loss: 0.6371 - val_accuracy: 0.5442 - val_loss: 0.7830\n",
            "Epoch 6/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.6945 - loss: 0.5888 - val_accuracy: 0.5013 - val_loss: 0.7482\n",
            "Epoch 7/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7423 - loss: 0.5344 - val_accuracy: 0.5308 - val_loss: 0.8011\n",
            "Epoch 8/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.7870 - loss: 0.4765 - val_accuracy: 0.5067 - val_loss: 0.8661\n",
            "Epoch 9/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8192 - loss: 0.4083 - val_accuracy: 0.4960 - val_loss: 0.8938\n",
            "Epoch 10/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8460 - loss: 0.3729 - val_accuracy: 0.5362 - val_loss: 0.9653\n",
            "Epoch 11/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8545 - loss: 0.3471 - val_accuracy: 0.5684 - val_loss: 0.9656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict for the test dataset\n",
        "test_transform = []\n",
        "for row in range(0, len(test.index)):\n",
        "    test_transform.append(' '.join(str(x) for x in test.iloc[row, 2:27]))\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_headlines)\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Predict for the test dataset\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "matrix = confusion_matrix(y_test, predicted_labels)\n",
        "print(\"Confusion Matrix:\\n\", matrix)\n",
        "score = accuracy_score(y_test, predicted_labels)\n",
        "print(\"Accuracy Score:\", score)\n",
        "report = classification_report(y_test, predicted_labels)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHsgkYZ1giXa",
        "outputId": "3b47835d-b239-46ba-cfd4-5f3ed8253db2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Confusion Matrix:\n",
            " [[144  42]\n",
            " [ 68 124]]\n",
            "Accuracy Score: 0.708994708994709\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.77      0.72       186\n",
            "           1       0.75      0.65      0.69       192\n",
            "\n",
            "    accuracy                           0.71       378\n",
            "   macro avg       0.71      0.71      0.71       378\n",
            "weighted avg       0.71      0.71      0.71       378\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec:\n",
        "        embedding_matrix[i] = word2vec[word]\n",
        "\n",
        "\n",
        "def stack_model(layer_name, epochs=50):\n",
        "    SEED = 42\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(word_index) + 1,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=MAX_SEQUENCE_LENGTH,\n",
        "                        trainable=False))\n",
        "    model.add(GRU(128, return_sequences=True))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(GRU(64))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=\"adam\", metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, callbacks=[early_stopping])\n",
        "    return model\n",
        "\n",
        "# Example usage with GRU layers\n",
        "model = stack_model(GRU, epochs=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "jN8j-Kh3kubq",
        "outputId": "c708712e-c7ca-49e2-ea60-86377a5073a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │       \u001b[38;5;34m6,314,100\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,314,100</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,314,100\u001b[0m (24.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,314,100</span> (24.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m6,314,100\u001b[0m (24.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,314,100</span> (24.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.4910 - loss: 0.7052 - val_accuracy: 0.5898 - val_loss: 0.6800\n",
            "Epoch 2/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.5412 - loss: 0.6868 - val_accuracy: 0.5818 - val_loss: 0.6772\n",
            "Epoch 3/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.5936 - loss: 0.6725 - val_accuracy: 0.5630 - val_loss: 0.6813\n",
            "Epoch 4/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.6198 - loss: 0.6435 - val_accuracy: 0.5871 - val_loss: 0.6896\n",
            "Epoch 5/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.6627 - loss: 0.6132 - val_accuracy: 0.5603 - val_loss: 0.7212\n",
            "Epoch 6/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.6965 - loss: 0.5770 - val_accuracy: 0.5335 - val_loss: 0.7462\n",
            "Epoch 7/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7414 - loss: 0.5200 - val_accuracy: 0.5603 - val_loss: 0.7888\n",
            "Epoch 8/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.7880 - loss: 0.4619 - val_accuracy: 0.5362 - val_loss: 0.8967\n",
            "Epoch 9/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8235 - loss: 0.3951 - val_accuracy: 0.5201 - val_loss: 1.1524\n",
            "Epoch 10/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8706 - loss: 0.3156 - val_accuracy: 0.5282 - val_loss: 1.5059\n",
            "Epoch 11/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8917 - loss: 0.2571 - val_accuracy: 0.5925 - val_loss: 1.5760\n",
            "Epoch 12/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9031 - loss: 0.2287 - val_accuracy: 0.5442 - val_loss: 1.7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict for the test dataset\n",
        "test_transform = []\n",
        "for row in range(0, len(test.index)):\n",
        "    test_transform.append(' '.join(str(x) for x in test.iloc[row, 2:27]))\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_headlines)\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Predict for the test dataset\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "matrix = confusion_matrix(y_test, predicted_labels)\n",
        "print(\"Confusion Matrix:\\n\", matrix)\n",
        "score = accuracy_score(y_test, predicted_labels)\n",
        "print(\"Accuracy Score:\", score)\n",
        "report = classification_report(y_test, predicted_labels)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSWY7V_nlK3_",
        "outputId": "d301e181-5c60-4cbe-bc6e-49cf12090271"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
            "Confusion Matrix:\n",
            " [[144  42]\n",
            " [ 68 124]]\n",
            "Accuracy Score: 0.708994708994709\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.77      0.72       186\n",
            "           1       0.75      0.65      0.69       192\n",
            "\n",
            "    accuracy                           0.71       378\n",
            "   macro avg       0.71      0.71      0.71       378\n",
            "weighted avg       0.71      0.71      0.71       378\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
        "traindataset = vectorizer.fit_transform(train_headlines)\n",
        "\n",
        "# Convert the sparse matrix to a dense format\n",
        "X_train = traindataset.toarray()\n",
        "\n",
        "# Sample target labels (replace with your actual labels)\n",
        "y_train = train['Label'].values\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define the model\n",
        "def stack_model(layer_name, epochs=50):\n",
        "    SEED = 42\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "\n",
        "    INPUT_DIM = X_train.shape[1]  # Number of features (ngrams)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, activation='relu', input_shape=(INPUT_DIM,)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=\"adam\", metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, callbacks=[early_stopping])\n",
        "    return model\n",
        "\n",
        "# Example usage with Dense layers\n",
        "model = stack_model(Dense, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "OOVBaXu7gpoK",
        "outputId": "0bde95cc-1f71-4abc-a48d-8c5de29731f1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │     \u001b[38;5;34m191,770,112\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m258\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │     <span style=\"color: #00af00; text-decoration-color: #00af00\">191,770,112</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m191,934,594\u001b[0m (732.17 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">191,934,594</span> (732.17 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m191,934,594\u001b[0m (732.17 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">191,934,594</span> (732.17 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.4864 - loss: 0.6935 - val_accuracy: 0.5871 - val_loss: 0.6828\n",
            "Epoch 2/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.7250 - loss: 0.5116 - val_accuracy: 0.4772 - val_loss: 0.9521\n",
            "Epoch 3/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.9941 - loss: 0.0124 - val_accuracy: 0.4424 - val_loss: 1.3660\n",
            "Epoch 4/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 7.8675e-04 - val_accuracy: 0.5228 - val_loss: 1.1049\n",
            "Epoch 5/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 1.4333e-04 - val_accuracy: 0.5201 - val_loss: 1.1007\n",
            "Epoch 6/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 5.6416e-05 - val_accuracy: 0.4960 - val_loss: 1.0951\n",
            "Epoch 7/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 2.2622e-04 - val_accuracy: 0.5174 - val_loss: 1.1133\n",
            "Epoch 8/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 2.7539e-05 - val_accuracy: 0.5094 - val_loss: 1.1098\n",
            "Epoch 9/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 1.2989e-05 - val_accuracy: 0.5147 - val_loss: 1.1092\n",
            "Epoch 10/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 1.3479e-05 - val_accuracy: 0.5040 - val_loss: 1.1108\n",
            "Epoch 11/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 7.7411e-06 - val_accuracy: 0.4987 - val_loss: 1.1127\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Predict for the test dataset\n",
        "test_transform = []\n",
        "for row in range(0, len(test.index)):\n",
        "    test_transform.append(' '.join(str(x) for x in test.iloc[row, 2:27]))\n",
        "test_dataset = vectorizer.transform(test_headlines)\n",
        "\n",
        "# Convert the sparse matrix to a dense format\n",
        "test_dataset = test_dataset.toarray()  # Convert to dense array\n",
        "\n",
        "predictions = model.predict(test_dataset)\n",
        "\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "matrix = confusion_matrix(test['Label'], predicted_labels)\n",
        "print(\"Confusion Matrix:\\n\", matrix)\n",
        "score = accuracy_score(test['Label'], predicted_labels)\n",
        "print(\"Accuracy Score:\", score)\n",
        "report = classification_report(test['Label'], predicted_labels)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on496CY6pAst",
        "outputId": "bb06f9ba-139a-4336-bd19-571991dd1311"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step\n",
            "Confusion Matrix:\n",
            " [[138  48]\n",
            " [ 44 148]]\n",
            "Accuracy Score: 0.7566137566137566\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.74      0.75       186\n",
            "           1       0.76      0.77      0.76       192\n",
            "\n",
            "    accuracy                           0.76       378\n",
            "   macro avg       0.76      0.76      0.76       378\n",
            "weighted avg       0.76      0.76      0.76       378\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Define the BERT-based model with custom classifier\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.drop = nn.Dropout(p=0.6)\n",
        "        self.fc1 = nn.Linear(self.bert.config.hidden_size, 128)\n",
        "        self.batch_norm = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.out = nn.Linear(32, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        cls_output = outputs.pooler_output\n",
        "        x = self.drop(cls_output)\n",
        "        x = self.fc1(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return self.out(x)\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 128\n",
        "N_CLASSES = 2\n",
        "LEARNING_RATE = 1e-5\n",
        "\n",
        "# Prepare data\n",
        "#X_train, X_val, y_train, y_val = train_test_split(train_headlines, train['Label'].values, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TextDataset(train_headlines, y_train, tokenizer, MAX_LEN)\n",
        "val_dataset = TextDataset(test_headlines, y_test, tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = BertClassifier(N_CLASSES)\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, criterion, optimizer, device, n_examples):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    for d in data_loader:\n",
        "        input_ids = d['input_ids'].to(device)\n",
        "        attention_mask = d['attention_mask'].to(device)\n",
        "        labels = d['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "# Evaluation function\n",
        "def eval_model(model, data_loader, criterion, device, n_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d['input_ids'].to(device)\n",
        "            attention_mask = d['attention_mask'].to(device)\n",
        "            labels = d['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n",
        "for epoch in range(EPOCHS):\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        device,\n",
        "        len(train_dataset)\n",
        "    )\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        val_loader,\n",
        "        criterion,\n",
        "        device,\n",
        "        len(val_dataset)\n",
        "    )\n",
        "\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print(f'Train loss: {train_loss}, accuracy: {train_acc}')\n",
        "    print(f'Val loss: {val_loss}, accuracy: {val_acc}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LSbYtnKARup",
        "outputId": "16f3d0b4-ab11-4337-9e4e-3d70251c42e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "Train loss: 0.8448635451814048, accuracy: 0.5072463768115942\n",
            "Val loss: 0.6954984615246455, accuracy: 0.5079365079365079\n",
            "Epoch 2/15\n",
            "Train loss: 0.8282691319274087, accuracy: 0.4975845410628019\n",
            "Val loss: 0.6981827095150948, accuracy: 0.5079365079365079\n",
            "Epoch 3/15\n",
            "Train loss: 0.8359262400712723, accuracy: 0.49704777241009124\n",
            "Val loss: 0.6966136594613394, accuracy: 0.5079365079365079\n",
            "Epoch 4/15\n",
            "Train loss: 0.812089121239817, accuracy: 0.5254965110037574\n",
            "Val loss: 0.696066771944364, accuracy: 0.5079365079365079\n",
            "Epoch 5/15\n",
            "Train loss: 0.7974694992741967, accuracy: 0.5373054213633923\n",
            "Val loss: 0.6983843122919401, accuracy: 0.5079365079365079\n",
            "Epoch 6/15\n",
            "Train loss: 0.8060865712980939, accuracy: 0.5249597423510467\n",
            "Val loss: 0.6986626436312994, accuracy: 0.5079365079365079\n",
            "Epoch 7/15\n",
            "Train loss: 0.8212464054425558, accuracy: 0.5072463768115942\n",
            "Val loss: 0.6975065469741821, accuracy: 0.5079365079365079\n",
            "Epoch 8/15\n",
            "Train loss: 0.8429134910942143, accuracy: 0.4975845410628019\n",
            "Val loss: 0.6997647856672605, accuracy: 0.5079365079365079\n",
            "Epoch 9/15\n",
            "Train loss: 0.8350724873379765, accuracy: 0.511003757380569\n",
            "Val loss: 0.6984715585907301, accuracy: 0.5079365079365079\n",
            "Epoch 10/15\n",
            "Train loss: 0.834375837419787, accuracy: 0.509393451422437\n",
            "Val loss: 0.6958356002966563, accuracy: 0.5079365079365079\n",
            "Epoch 11/15\n",
            "Train loss: 0.8071192564108433, accuracy: 0.5195920558239399\n",
            "Val loss: 0.6973765542109808, accuracy: 0.5079365079365079\n",
            "Epoch 12/15\n",
            "Train loss: 0.8228662192312062, accuracy: 0.5104669887278583\n",
            "Val loss: 0.6967587769031525, accuracy: 0.5079365079365079\n",
            "Epoch 13/15\n",
            "Train loss: 0.816835219024593, accuracy: 0.5056360708534622\n",
            "Val loss: 0.6944344341754913, accuracy: 0.5079365079365079\n",
            "Epoch 14/15\n",
            "Train loss: 0.826614478459725, accuracy: 0.512614063338701\n",
            "Val loss: 0.6978629579146703, accuracy: 0.5079365079365079\n",
            "Epoch 15/15\n",
            "Train loss: 0.8010952276551825, accuracy: 0.5072463768115942\n",
            "Val loss: 0.6957370166977247, accuracy: 0.5079365079365079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict for the test dataset\n",
        "test_transform = []\n",
        "for row in range(0, len(test.index)):\n",
        "    test_transform.append(' '.join(str(x) for x in test.iloc[row, 2:27]))\n",
        "\n",
        "test_dataset = TextDataset(test_headlines, test['Label'].values, tokenizer, MAX_LEN)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "real_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for d in test_loader:\n",
        "        input_ids = d['input_ids'].to(device)\n",
        "        attention_mask = d['attention_mask'].to(device)\n",
        "        labels = d['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        predictions.extend(preds)\n",
        "        real_values.extend(labels)\n",
        "\n",
        "predictions = torch.stack(predictions).cpu()\n",
        "real_values = torch.stack(real_values).cpu()\n",
        "\n",
        "matrix = confusion_matrix(real_values, predictions)\n",
        "print(\"Confusion Matrix:\\n\", matrix)\n",
        "score = accuracy_score(real_values, predictions)\n",
        "print(\"Accuracy Score:\", score)\n",
        "report = classification_report(real_values, predictions)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCkTiZmrAx6H",
        "outputId": "c1692c10-fbce-4994-c522-33a2b23f9e78"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[  0 186]\n",
            " [  0 192]]\n",
            "Accuracy Score: 0.5079365079365079\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       186\n",
            "           1       0.51      1.00      0.67       192\n",
            "\n",
            "    accuracy                           0.51       378\n",
            "   macro avg       0.25      0.50      0.34       378\n",
            "weighted avg       0.26      0.51      0.34       378\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Load FinBERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "# Function to get sentiment\n",
        "def get_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    return torch.argmax(probs, dim=1).item()\n",
        "\n",
        "# Predict sentiments for train and test datasets\n",
        "train_labels = [get_sentiment(headline) for headline in train_headlines]\n",
        "test_labels = [get_sentiment(headline) for headline in test_headlines]\n",
        "\n",
        "# True labels\n",
        "y_train_true = train['Label'].tolist()\n",
        "y_test_true = test['Label'].tolist()\n",
        "\n",
        "# Print results\n",
        "print(\"Training set shape:\", len(train_headlines))\n",
        "print(\"Testing set shape:\", len(test_headlines))\n",
        "\n",
        "# Print a sample of the preprocessed headlines and their predicted sentiments\n",
        "print(\"\\nSample of preprocessed headlines and predicted sentiments (train):\")\n",
        "for i in range(2):\n",
        "    print(f\"Headline: {train_headlines[i]}\")\n",
        "    print(f\"Predicted Sentiment: {train_labels[i]}\")\n",
        "\n",
        "print(\"\\nSample of preprocessed headlines and predicted sentiments (test):\")\n",
        "for i in range(2):\n",
        "    print(f\"Headline: {test_headlines[i]}\")\n",
        "    print(f\"Predicted Sentiment: {test_labels[i]}\")\n",
        "\n",
        "# Compute and print confusion matrix and classification report\n",
        "print(\"\\nConfusion Matrix (test):\")\n",
        "cm = confusion_matrix(y_test_true, test_labels)\n",
        "print(cm)\n",
        "\n",
        "print(\"\\nClassification Report (test):\")\n",
        "print(classification_report(y_test_true, test_labels))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test_true, test_labels)\n",
        "print(f\"\\nAccuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e026ec7479514d18a3ab79aecfce542d",
            "968122e36c864963b548850357465767",
            "f86e90a15f1d4b36811af6744483a0f1",
            "aac8607e3ac3484a8b0e28c09d9ca3ac",
            "f92003dfcd804988b148017aaaf3042a",
            "58a7c7eaeb234005b874fa877128905b",
            "9679a7b3319743dfa8339a1a1feb6a33",
            "d4287bf07fd646dab37e0f6634a319b8",
            "7858c985f6d6455383882179aaa23a0d",
            "f6796894e34f402882ce804a252e16c9",
            "1b8fe9d49e514bde9ab141adb6772b8c",
            "cc9b7de2eae74f9ba6ee9b487cd6a1ee",
            "ade1c9ecc0674781bab72d5072b756c7",
            "dc0391c1f0e84c0abaa1bdbc382ecccc",
            "84ec8168402e4455afea2b91791c45fb",
            "42c27703b3dd4a2da0d5bc3fe805fbf7",
            "6900716ed6804304bb2823279ba86ab2",
            "78455f98eb9245d6ad500f859bd2bd1b",
            "033fb228b0364e1f81fcda263929ac78",
            "d527fc5b7cd74a309c21db6ff1180444",
            "387f17286656492e85e8a942390660d5",
            "45469eaf2b014dcd874fc0ec3ee4e3f9",
            "93058efaa14a47eeb14f87fb7e8effc8",
            "6e63a4f6562c47aca94f735e2b37ea87",
            "d47dee0a15284d469c2f38998ec44928",
            "cf02c5c153a34ec6bb657c158cf3e424",
            "83ba25ec93b040cf998b367177c5c8a8",
            "13f7920691f741cfaa2202bc5a58cba0",
            "9e8a856d2ba2455abc3c538c82be7bda",
            "260ea9a5dc5543caaa2d77dcc5288ddc",
            "c34cd23d9d00454c93fdb2fb9e12dcba",
            "37660e3aad1549e59a56c552f9343797",
            "8b47e29c5c264068a2d7f99c9f13b504"
          ]
        },
        "id": "bIkIAS8U-YJZ",
        "outputId": "ab81921a-771b-4c84-acc5-997f23b14ef1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e026ec7479514d18a3ab79aecfce542d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc9b7de2eae74f9ba6ee9b487cd6a1ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93058efaa14a47eeb14f87fb7e8effc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: 1863\n",
            "Testing set shape: 378\n",
            "\n",
            "Sample of preprocessed headlines and predicted sentiments (train):\n",
            "Headline: b georgia down two russian warplan countri move brink war b break musharraf impeach b russia today column troop roll south ossetia footag fight youtub b russian tank move toward capit south ossetia reportedli complet destroy georgian artilleri fire b afghan child rape impun u n offici say sick three year old rape noth b russian tank enter south ossetia whilst georgia shoot two russian jet b break georgia invad south ossetia russia warn would interven side b enemi combat trial noth sham salim haman sentenc year kept longer anyway feel like b georgian troop retreat osettain capit presum leav sever hundr peopl kill video b u prep georgia war russia b rice give green light israel attack iran say u veto isra militari op b announc class action lawsuit behalf american public fbi b russia georgia war nyt top stori open ceremoni olymp fuck disgrac yet proof declin journal b china tell bush stay countri affair b world war iii start today b georgia invad south ossetia russia get involv nato absorb georgia unleash full scale war b al qaeda face islamist backlash b condoleezza rice u would act prevent isra strike iran isra defens minist ehud barak israel prepar uncompromis victori case militari hostil b busi day european union approv new sanction iran protest nuclear programm b georgia withdraw soldier iraq help fight russian forc georgia breakaway region south ossetia b pentagon think attack iran bad idea u news amp world report b caucasu crisi georgia invad south ossetia b indian shoe manufactori seri like work b visitor suffer mental ill ban olymp b help mexico kidnap surg\n",
            "Predicted Sentiment: 0\n",
            "Headline: b wont america nato help u wont help u help iraq b bush put foot georgian conflict b jewish georgian minist thank isra train fend russia b georgian armi flee disarray russian advanc gori abandon russia without shot fire b olymp open ceremoni firework fake b mossad fraudul new zealand passport iraq b russia anger isra militari sale georgia b american citizen live ossetia blame u georgian leader genocid innoc peopl b welcom world war iv high definit b georgia move mistak monument proport b russia press deeper georgia u say regim chang goal b abhinav bindra win first ever individu olymp gold medal india b u ship head arctic defin territori b driver jerusalem taxi station threaten quit rather work new bos arab b french team stun phelp x relay team b israel u behind georgian aggress b believ tv neither russian georgian much victim b riot still go montreal canada polic murder boy saturday b china overtak u largest manufactur b war south ossetia pic b isra physician group condemn state tortur b russia beaten unit state head peak oil b perhap question georgia russia conflict b russia much better war b come trade sex food\n",
            "Predicted Sentiment: 0\n",
            "\n",
            "Sample of preprocessed headlines and predicted sentiments (test):\n",
            "Headline: case cancer result sheer bad luck rather unhealthi lifestyl diet even inherit gene new research suggest random mutat occur dna cell divid respons two third adult cancer across wide rang tissu iran dismiss unit state effort fight islam state ploy advanc u polici region realiti unit state act elimin daesh even interest weaken daesh interest manag poll one german would join anti muslim march uk royal famili princ andrew name u lawsuit underag sex alleg asylum seeker refus leav bu arriv destin rural northern sweden demand taken back malm big citi pakistani boat blow self india navi chase four peopl board vessel near pakistani port citi karachi believ kill dramat episod arabian sea new year eve accord india defenc ministri sweden hit third mosqu arson attack week car set alight french new year salari top ceo rose twice fast averag canadian sinc recess studi norway violat equal pay law judg say judg find consul employe unjustli paid le male counterpart imam want radic recruit muslim youth canada identifi dealt saudi arabia behead peopl year live hell slave remot south korean island slaveri thrive chain rural island south korea rug southwest coast nurtur long histori exploit demand tri squeez live sea world richest get richer ad bn rental car stereo infring copyright music right group say ukrainian minist threaten tv channel closur air russian entertain palestinian presid mahmoud abba enter seriou confront yet israel sign onto intern crimin court decis wednesday give court jurisdict crime commit palestinian land isra secur center publish name kill terrorist conceal hama year deadliest year yet syria four year conflict kill secret underground complex built nazi may use develop wmd includ nuclear bomb uncov austria restrict web freedom major global issu austrian journalist erich mchel deliv present hamburg annual meet chao comput club monday decemb detail variou locat u nsa activ collect process electron intellig vienna thousand ukrain nationalist march kiev china new year resolut harvest execut prison organ author pull plug russia last polit independ tv station\n",
            "Predicted Sentiment: 0\n",
            "Headline: moscow gt beij high speed train reduc trip time hour current day two ancient tomb discov egypt sunday one repres symbol burial site god osiri anoth claim previous unknown pharaon tomb china complain pyongyang n korean soldier kill villag world news scotland head toward fossil fuel free renew energi project combin energi save could decarbonis countri power sector need see phase convent gener scotland prime minist shinzo abe said monday express remors japan action world war ii highlight countri bid contribut activ world peac statement mark th anniversari war end august sex slave centr princ andrew scandal fled australia teen gay rel hama founder face deport canada christian convert whose five uncl jail israel terror activ say hell kill forc return west bank number femal drug addict iran almost doubl sinc start age decad search causeway great pyramid egypt found india lost tiger lizard squad hack soni huge market scheme launch lizard stresser paid hack servic russia may face chao extra sanction impos germani saudi air land forc carri attack isi within territori isil reportedli set bank attempt legitimatis nation state israel brand anti assimil group lehava terrorist islam state approv budget billion expect surplu million open islam bank mosul budget reportedli includ monthli wage poor disabl famili individu kill airstrik carri u led coalit forc iceland withdraw eu applic lift capit control blackfield capit founder goe miss valu rubl thing vanish russia moscow hedg fund chief execut disappear along money firm account rocket stage crash back earth rural chine villag dead aircraft bomb greek tanker libyan port belgian murder frank van den bleeken die request belgian murder rapist serv life sentenc allow die lethal inject next sunday follow rule law belgium permit peopl request euthanasia czech presid critic ukrainian pm say yatsenyuk prime minist war vietnames jet join search miss filipino bahamian cargo ship sink franc seek end russia sanction ukrain china scrap rare earth cap\n",
            "Predicted Sentiment: 0\n",
            "\n",
            "Confusion Matrix (test):\n",
            "[[186   0]\n",
            " [192   0]]\n",
            "\n",
            "Classification Report (test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      1.00      0.66       186\n",
            "           1       0.00      0.00      0.00       192\n",
            "\n",
            "    accuracy                           0.49       378\n",
            "   macro avg       0.25      0.50      0.33       378\n",
            "weighted avg       0.24      0.49      0.32       378\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch sklearn pandas seaborn matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK9rS05I_wlQ",
        "outputId": "4c529058-90b3-4a64-926a-ea3eedef2122"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, headlines, labels, tokenizer, max_len):\n",
        "        self.headlines = headlines\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.headlines)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        headline = str(self.headlines[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            headline,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'headline_text': headline,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def create_data_loader(headlines, labels, tokenizer, max_len, batch_size):\n",
        "    ds = NewsDataset(\n",
        "        headlines=headlines,\n",
        "        labels=labels,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 128\n",
        "\n",
        "train_data_loader = create_data_loader(train_headlines, y_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(test_headlines, y_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# Model\n",
        "class NewsClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(NewsClassifier, self).__init__()\n",
        "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "model = NewsClassifier(n_classes=3)\n",
        "model = model.to(device)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "    model = model.train()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "# Evaluation function\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "    model = model.eval()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            labels = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "EPOCHS = 15\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_data_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler,\n",
        "        len(train_headlines)\n",
        "    )\n",
        "\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        test_data_loader,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(test_headlines)\n",
        "    )\n",
        "\n",
        "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
        "    print()\n",
        "\n",
        "# Evaluation on test set\n",
        "y_test_pred = []\n",
        "y_test_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for d in test_data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        y_test_pred.extend(preds)\n",
        "        y_test_true.extend(labels)\n",
        "\n",
        "y_test_pred = torch.stack(y_test_pred).cpu()\n",
        "y_test_true = torch.stack(y_test_true).cpu()\n",
        "\n",
        "# Compute and print confusion matrix and classification report\n",
        "print(\"\\nConfusion Matrix (test):\")\n",
        "cm = confusion_matrix(y_test_true, y_test_pred)\n",
        "print(cm)\n",
        "\n",
        "print(\"\\nClassification Report (test):\")\n",
        "print(classification_report(y_test_true, y_test_pred))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test_true, y_test_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sgQE5_fl_xhR",
        "outputId": "ee494c4e-cf64-44cb-a5ca-98088778ad2c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7185701702395056 accuracy 0.533011272141707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6981853743394216 accuracy 0.49206349206349204\n",
            "\n",
            "Epoch 2/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7028585157842717 accuracy 0.5217391304347826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6948435256878535 accuracy 0.49206349206349204\n",
            "\n",
            "Epoch 3/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6965077081297197 accuracy 0.5297906602254429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.693998264769713 accuracy 0.49206349206349204\n",
            "\n",
            "Epoch 4/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6938706580390278 accuracy 0.5314009661835749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6949303473035494 accuracy 0.49206349206349204\n",
            "\n",
            "Epoch 5/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6942841874228584 accuracy 0.5453569511540526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.687168208261331 accuracy 0.5105820105820106\n",
            "\n",
            "Epoch 6/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6595703793896569 accuracy 0.6129898013955984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.588474610199531 accuracy 0.7248677248677249\n",
            "\n",
            "Epoch 7/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5352546949671884 accuracy 0.7342995169082126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.589441924666365 accuracy 0.7142857142857142\n",
            "\n",
            "Epoch 8/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.4043920051593047 accuracy 0.8287707997852926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6024105719601115 accuracy 0.7751322751322751\n",
            "\n",
            "Epoch 9/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.32097480716740984 accuracy 0.8727858293075684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.637989541515708 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 10/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.28489475436190254 accuracy 0.8904991948470209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6557809567699829 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 11/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.2644814945375308 accuracy 0.9006977992485239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6557809567699829 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 12/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.26233502267262876 accuracy 0.8974771873322598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6557809567699829 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 13/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.27074521655837697 accuracy 0.8947933440687064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6557809567699829 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 14/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.2683644233924201 accuracy 0.895330112721417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6557809567699829 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 15/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.26998639673504055 accuracy 0.8964036500268384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6557809567699829 accuracy 0.7777777777777777\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix (test):\n",
            "[[140  46]\n",
            " [ 38 154]]\n",
            "\n",
            "Classification Report (test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.75      0.77       186\n",
            "           1       0.77      0.80      0.79       192\n",
            "\n",
            "    accuracy                           0.78       378\n",
            "   macro avg       0.78      0.78      0.78       378\n",
            "weighted avg       0.78      0.78      0.78       378\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, headlines, labels, tokenizer, max_len):\n",
        "        self.headlines = headlines\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.headlines)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        headline = str(self.headlines[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            headline,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'headline_text': headline,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def create_data_loader(headlines, labels, tokenizer, max_len, batch_size):\n",
        "    ds = NewsDataset(\n",
        "        headlines=headlines,\n",
        "        labels=labels,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 128\n",
        "\n",
        "train_data_loader = create_data_loader(train_headlines, y_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(test_headlines, y_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# Model\n",
        "class NewsClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(NewsClassifier, self).__init__()\n",
        "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "model = NewsClassifier(n_classes=2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "EPOCHS = 15\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "    model = model.train()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "# Evaluation function\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "    model = model.eval()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            labels = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "EPOCHS = 15\n",
        "PATIENCE = 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_accuracy = 0\n",
        "best_model = None\n",
        "patience_counter = 0\n",
        "\n",
        "# Training loop with early stopping\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_data_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler,\n",
        "        len(train_headlines)\n",
        "    )\n",
        "\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        test_data_loader,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(test_headlines)\n",
        "    )\n",
        "\n",
        "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
        "\n",
        "    # Check for early stopping\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_accuracy = val_acc\n",
        "        best_model = model.state_dict()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# Evaluation on test set\n",
        "y_test_pred = []\n",
        "y_test_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for d in test_data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        y_test_pred.extend(preds)\n",
        "        y_test_true.extend(labels)\n",
        "\n",
        "y_test_pred = torch.stack(y_test_pred).cpu()\n",
        "y_test_true = torch.stack(y_test_true).cpu()\n",
        "\n",
        "# Compute and print confusion matrix and classification report\n",
        "print(\"\\nConfusion Matrix (test):\")\n",
        "cm = confusion_matrix(y_test_true, y_test_pred)\n",
        "print(cm)\n",
        "\n",
        "print(\"\\nClassification Report (test):\")\n",
        "print(classification_report(y_test_true, y_test_pred))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test_true, y_test_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "c9ucqa3sMuEM",
        "outputId": "8ed45fd2-4bde-486b-8a7d-6ee9ff11ea9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7021188603507148 accuracy 0.5324745034889963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6939821690320969 accuracy 0.49206349206349204\n",
            "Epoch 2/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6997405560607584 accuracy 0.5319377348362856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6940748766064644 accuracy 0.49206349206349204\n",
            "Epoch 3/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7073862481321025 accuracy 0.499194847020934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6928532098730406 accuracy 0.5079365079365079\n",
            "Epoch 4/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7043645224000654 accuracy 0.49221685453569514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.692837248245875 accuracy 0.5079365079365079\n",
            "Epoch 5/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7018523975315257 accuracy 0.49973161567364466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6926012163360914 accuracy 0.5079365079365079\n",
            "Epoch 6/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7012503544489542 accuracy 0.5179817498658078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6944740439454714 accuracy 0.5317460317460317\n",
            "Epoch 7/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6978535514611465 accuracy 0.5421363392377885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6924864500761032 accuracy 0.5132275132275133\n",
            "Epoch 8/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6543414289625282 accuracy 0.6210413311862587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6546112845341364 accuracy 0.6058201058201058\n",
            "Epoch 9/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5838153737987208 accuracy 0.7090713902308106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6199272026618322 accuracy 0.6904761904761905\n",
            "Epoch 10/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.4843266547108308 accuracy 0.7852925389157274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5931653374185165 accuracy 0.7195767195767195\n",
            "Epoch 11/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.3938775991336403 accuracy 0.8513150831991412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.685376174437503 accuracy 0.7513227513227513\n",
            "Epoch 12/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.3150087219432124 accuracy 0.8921095008051529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.701973515873154 accuracy 0.7724867724867724\n",
            "Epoch 13/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.3282294515042733 accuracy 0.895330112721417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.7890253516379744 accuracy 0.7671957671957672\n",
            "Epoch 14/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.2497660938061328 accuracy 0.9253891572732152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.779722617745089 accuracy 0.7883597883597884\n",
            "Epoch 15/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.24672061232770356 accuracy 0.927536231884058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.7897240435316538 accuracy 0.7989417989417988\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix (test):\n",
            "[[136  50]\n",
            " [ 26 166]]\n",
            "\n",
            "Classification Report (test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.73      0.78       186\n",
            "           1       0.77      0.86      0.81       192\n",
            "\n",
            "    accuracy                           0.80       378\n",
            "   macro avg       0.80      0.80      0.80       378\n",
            "weighted avg       0.80      0.80      0.80       378\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "id": "uG5DgsDzN8s4",
        "outputId": "eb713903-ee03-4df8-fa16-4d80521ca922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import optuna\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "\n",
        "# Suppress specific warning messages\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# Set logging level for transformers and other libraries\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, headlines, labels, tokenizer, max_len):\n",
        "        self.headlines = headlines\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.headlines)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        headline = str(self.headlines[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            headline,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,  # Explicitly specify truncation\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'headline_text': headline,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def create_data_loader(headlines, labels, tokenizer, max_len, batch_size):\n",
        "    ds = NewsDataset(\n",
        "        headlines=headlines,\n",
        "        labels=labels,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "# Model\n",
        "class NewsClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(NewsClassifier, self).__init__()\n",
        "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            labels = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameters\n",
        "    BATCH_SIZE = trial.suggest_int(\"batch_size\", 8, 32, step=8)\n",
        "    MAX_LEN = trial.suggest_int(\"max_len\", 64, 256, step=64)\n",
        "    LEARNING_RATE = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
        "    EPOCHS = trial.suggest_int(\"epochs\", 3, 10)\n",
        "\n",
        "    print(f\"Trial {trial.number}: batch_size={BATCH_SIZE}, max_len={MAX_LEN}, learning_rate={LEARNING_RATE}, epochs={EPOCHS}\")\n",
        "\n",
        "    # Data loaders\n",
        "    train_data_loader = create_data_loader(train_headlines, y_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "    test_data_loader = create_data_loader(test_headlines, y_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = NewsClassifier(n_classes=2)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
        "    total_steps = len(train_data_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_accuracy = 0\n",
        "    patience_counter = 0\n",
        "    PATIENCE = 3\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_acc, train_loss = train_epoch(\n",
        "            model,\n",
        "            train_data_loader,\n",
        "            loss_fn,\n",
        "            optimizer,\n",
        "            device,\n",
        "            scheduler,\n",
        "            len(train_headlines)\n",
        "        )\n",
        "\n",
        "        val_acc, val_loss = eval_model(\n",
        "            model,\n",
        "            test_data_loader,\n",
        "            loss_fn,\n",
        "            device,\n",
        "            len(test_headlines)\n",
        "        )\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS} - Train loss: {train_loss:.4f}, Train accuracy: {train_acc:.4f}, Val loss: {val_loss:.4f}, Val accuracy: {val_acc:.4f}\")\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_accuracy = val_acc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    return best_accuracy\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(f\"\\nBest hyperparameters: {study.best_params}\")\n",
        "print(f\"Best trial accuracy: {study.best_trial.value}\")\n",
        "\n",
        "# Use best hyperparameters to create final model\n",
        "best_params = study.best_params\n",
        "BATCH_SIZE = best_params[\"batch_size\"]\n",
        "MAX_LEN = best_params[\"max_len\"]\n",
        "LEARNING_RATE = best_params[\"learning_rate\"]\n",
        "EPOCHS = best_params[\"epochs\"]\n",
        "\n",
        "train_data_loader = create_data_loader(train_headlines, y_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(test_headlines, y_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = NewsClassifier(n_classes=2)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_accuracy = 0\n",
        "best_model = None\n",
        "patience_counter = 0\n",
        "PATIENCE = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_data_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler,\n",
        "        len(train_headlines)\n",
        "    )\n",
        "\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        test_data_loader,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(test_headlines)\n",
        "    )\n",
        "\n",
        "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_accuracy = val_acc\n",
        "        best_model = model.state_dict()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "y_test_pred = []\n",
        "y_test_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for d in test_data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        y_test_pred.extend(preds)\n",
        "        y_test_true.extend(labels)\n",
        "\n",
        "y_test_pred = torch.stack(y_test_pred).cpu()\n",
        "y_test_true = torch.stack(y_test_true).cpu()\n",
        "\n",
        "print(classification_report(y_test_true, y_test_pred, target_names=['negative', 'positive']))\n",
        "\n",
        "cf_matrix = confusion_matrix(y_test_true, y_test_pred)\n",
        "sns.heatmap(cf_matrix, annot=True, fmt='d')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zGOa8yj1OBMm",
        "outputId": "9fc7346b-78dc-4036-ab96-16bef9214df5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-03 22:14:46,698] A new study created in memory with name: no-name-223621ad-40aa-4c7c-8d1f-67160e4895ed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0: batch_size=24, max_len=192, learning_rate=2.242287848775383e-05, epochs=6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6 - Train loss: 0.7009, Train accuracy: 0.5153, Val loss: 0.6969, Val accuracy: 0.4921\n",
            "Epoch 2/6 - Train loss: 0.6948, Train accuracy: 0.5239, Val loss: 0.6931, Val accuracy: 0.5079\n",
            "Epoch 3/6 - Train loss: 0.6947, Train accuracy: 0.5078, Val loss: 0.6936, Val accuracy: 0.5079\n",
            "Epoch 4/6 - Train loss: 0.6920, Train accuracy: 0.5282, Val loss: 0.6949, Val accuracy: 0.5079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-03 22:18:13,990] Trial 0 finished with value: 0.5079365079365079 and parameters: {'batch_size': 24, 'max_len': 192, 'learning_rate': 2.242287848775383e-05, 'epochs': 6}. Best is trial 0 with value: 0.5079365079365079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/6 - Train loss: 0.6936, Train accuracy: 0.5185, Val loss: 0.6954, Val accuracy: 0.5079\n",
            "Early stopping triggered\n",
            "Trial 1: batch_size=16, max_len=192, learning_rate=6.907268349663886e-05, epochs=6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6 - Train loss: 0.7073, Train accuracy: 0.5303, Val loss: 0.7002, Val accuracy: 0.4921\n",
            "Epoch 2/6 - Train loss: 0.7031, Train accuracy: 0.5191, Val loss: 0.6953, Val accuracy: 0.4921\n",
            "Epoch 3/6 - Train loss: 0.7008, Train accuracy: 0.5395, Val loss: 0.6957, Val accuracy: 0.4921\n",
            "Epoch 4/6 - Train loss: 0.7029, Train accuracy: 0.5110, Val loss: 0.6929, Val accuracy: 0.5079\n",
            "Epoch 5/6 - Train loss: 0.7053, Train accuracy: 0.5019, Val loss: 0.6932, Val accuracy: 0.5079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-03 22:22:19,017] Trial 1 finished with value: 0.5079365079365079 and parameters: {'batch_size': 16, 'max_len': 192, 'learning_rate': 6.907268349663886e-05, 'epochs': 6}. Best is trial 0 with value: 0.5079365079365079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/6 - Train loss: 0.6968, Train accuracy: 0.5099, Val loss: 0.6945, Val accuracy: 0.5079\n",
            "Trial 2: batch_size=24, max_len=192, learning_rate=0.0005701498501659254, epochs=7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    }
  ]
}